{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback Prize Effectiveness EDA and DeBERTa baseline\n",
    "\n",
    "**Before you fork, please give this notebook an upvote ðŸ™‚**\n",
    "\n",
    "Welcome to yet another NLP Kaggle competition! For those of us to enjoy NLP competitions, this is a great time, with 3 NLP competitions occuring simultaneously!\n",
    "\n",
    "Today I'll walk you through this new competition: **Feedback Prize - Predicting Effective Arguments**\n",
    "\n",
    "In this competition, we are given a dataset that contains argumentative essays written by U.S students in grades 6-12. As we will see, these essays were annotated by expert raters for _discourse elements_ commonly found in argumentative writing. We are tasked to predict the effectiveness of these discourse elements in the essays. The goal is that such a predictive model can provide automated feedback to students to help them become more confident, proficient writers.\n",
    "\n",
    "Let's get started with a detailed EDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:21.425914Z",
     "iopub.status.busy": "2022-06-19T13:18:21.425589Z",
     "iopub.status.idle": "2022-06-19T13:18:21.458258Z",
     "shell.execute_reply": "2022-06-19T13:18:21.457604Z",
     "shell.execute_reply.started": "2022-06-19T13:18:21.425829Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n",
    "if not iskaggle:\n",
    "    import zipfile,kaggle\n",
    "    path = Path('feedback-prize-effectiveness')\n",
    "    kaggle.api.competition_download_cli(str(path))\n",
    "    zipfile.ZipFile(f'{path}.zip').extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:21.462717Z",
     "iopub.status.busy": "2022-06-19T13:18:21.459497Z",
     "iopub.status.idle": "2022-06-19T13:18:21.839016Z",
     "shell.execute_reply": "2022-06-19T13:18:21.838319Z",
     "shell.execute_reply.started": "2022-06-19T13:18:21.462678Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what is available for us to work with for this competition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:21.840489Z",
     "iopub.status.busy": "2022-06-19T13:18:21.840256Z",
     "iopub.status.idle": "2022-06-19T13:18:21.851261Z",
     "shell.execute_reply": "2022-06-19T13:18:21.850229Z",
     "shell.execute_reply.started": "2022-06-19T13:18:21.840458Z"
    }
   },
   "outputs": [],
   "source": [
    "if iskaggle: path = Path('../input/feedback-prize-effectiveness')\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our standard `train.csv`, `test.csv` and `sample_submission.csv` files along with a `train` and `test` folder.\n",
    "\n",
    "Let's look at `train.csv` more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:21.854036Z",
     "iopub.status.busy": "2022-06-19T13:18:21.853347Z",
     "iopub.status.idle": "2022-06-19T13:18:22.121073Z",
     "shell.execute_reply": "2022-06-19T13:18:22.12039Z",
     "shell.execute_reply.started": "2022-06-19T13:18:21.854Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:22.123976Z",
     "iopub.status.busy": "2022-06-19T13:18:22.123232Z",
     "iopub.status.idle": "2022-06-19T13:18:22.129702Z",
     "shell.execute_reply": "2022-06-19T13:18:22.128765Z",
     "shell.execute_reply.started": "2022-06-19T13:18:22.123938Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and the `test.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:22.131226Z",
     "iopub.status.busy": "2022-06-19T13:18:22.130871Z",
     "iopub.status.idle": "2022-06-19T13:18:22.151062Z",
     "shell.execute_reply": "2022-06-19T13:18:22.150398Z",
     "shell.execute_reply.started": "2022-06-19T13:18:22.131187Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(path/'test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:22.152476Z",
     "iopub.status.busy": "2022-06-19T13:18:22.152228Z",
     "iopub.status.idle": "2022-06-19T13:18:22.158086Z",
     "shell.execute_reply": "2022-06-19T13:18:22.157367Z",
     "shell.execute_reply.started": "2022-06-19T13:18:22.152444Z"
    }
   },
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through each column and make sure we understand them.\n",
    "\n",
    "...starting with `discourse_id`. This is just a unique ID for each \"discourse\" we are classifying. As we can see, the number of unique `discourse_id`s equals the number of rows in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:22.160187Z",
     "iopub.status.busy": "2022-06-19T13:18:22.159551Z",
     "iopub.status.idle": "2022-06-19T13:18:22.177662Z",
     "shell.execute_reply": "2022-06-19T13:18:22.176918Z",
     "shell.execute_reply.started": "2022-06-19T13:18:22.160096Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df.discourse_id.unique()) == len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at `essay_id`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:22.180391Z",
     "iopub.status.busy": "2022-06-19T13:18:22.180212Z",
     "iopub.status.idle": "2022-06-19T13:18:22.197982Z",
     "shell.execute_reply": "2022-06-19T13:18:22.197308Z",
     "shell.execute_reply.started": "2022-06-19T13:18:22.180369Z"
    }
   },
   "outputs": [],
   "source": [
    "df.essay_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are about 4k essays in the dataset, and some essays have many data rows/annotations corresponding to them, while others might only have one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:22.199368Z",
     "iopub.status.busy": "2022-06-19T13:18:22.199138Z",
     "iopub.status.idle": "2022-06-19T13:18:22.218111Z",
     "shell.execute_reply": "2022-06-19T13:18:22.217431Z",
     "shell.execute_reply.started": "2022-06-19T13:18:22.199336Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df.essay_id=='91B1F82B2CF1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:22.219445Z",
     "iopub.status.busy": "2022-06-19T13:18:22.219135Z",
     "iopub.status.idle": "2022-06-19T13:18:22.237771Z",
     "shell.execute_reply": "2022-06-19T13:18:22.237062Z",
     "shell.execute_reply.started": "2022-06-19T13:18:22.21941Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df.essay_id=='DECAE402BB38'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:22.23934Z",
     "iopub.status.busy": "2022-06-19T13:18:22.23909Z",
     "iopub.status.idle": "2022-06-19T13:18:22.255774Z",
     "shell.execute_reply": "2022-06-19T13:18:22.254894Z",
     "shell.execute_reply.started": "2022-06-19T13:18:22.239307Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df.essay_id=='9706F8E7D534']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:22.257622Z",
     "iopub.status.busy": "2022-06-19T13:18:22.257153Z",
     "iopub.status.idle": "2022-06-19T13:18:22.277119Z",
     "shell.execute_reply": "2022-06-19T13:18:22.276408Z",
     "shell.execute_reply.started": "2022-06-19T13:18:22.257441Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df.essay_id=='5E85F1FB4E22']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data more closely, it seems like the essays with more data points have basically been annotated very carefully, with like each sentence of the essay having an annotation. On the other hand, when it only has one annotation, it looks like the whole essay is given one annotation. This should be further analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the essays are available in full in the folders (`train` for the training set and `test` for the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:22.282422Z",
     "iopub.status.busy": "2022-06-19T13:18:22.281543Z",
     "iopub.status.idle": "2022-06-19T13:18:22.416239Z",
     "shell.execute_reply": "2022-06-19T13:18:22.415518Z",
     "shell.execute_reply.started": "2022-06-19T13:18:22.282384Z"
    }
   },
   "outputs": [],
   "source": [
    "len(os.listdir(path/'train')) == len(df.essay_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:22.417837Z",
     "iopub.status.busy": "2022-06-19T13:18:22.417534Z",
     "iopub.status.idle": "2022-06-19T13:18:22.427481Z",
     "shell.execute_reply": "2022-06-19T13:18:22.42664Z",
     "shell.execute_reply.started": "2022-06-19T13:18:22.417798Z"
    }
   },
   "outputs": [],
   "source": [
    "len(os.listdir(path/'test')) == len(test_df.essay_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:22.429378Z",
     "iopub.status.busy": "2022-06-19T13:18:22.429033Z",
     "iopub.status.idle": "2022-06-19T13:18:34.819065Z",
     "shell.execute_reply": "2022-06-19T13:18:34.818298Z",
     "shell.execute_reply.started": "2022-06-19T13:18:22.429343Z"
    }
   },
   "outputs": [],
   "source": [
    "essay_len = []\n",
    "for file in os.listdir(path/'train'):\n",
    "    with open(path/'train'/file) as f:\n",
    "        essay_len.append(len(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:34.822022Z",
     "iopub.status.busy": "2022-06-19T13:18:34.821582Z",
     "iopub.status.idle": "2022-06-19T13:18:34.829152Z",
     "shell.execute_reply": "2022-06-19T13:18:34.828364Z",
     "shell.execute_reply.started": "2022-06-19T13:18:34.821982Z"
    }
   },
   "outputs": [],
   "source": [
    "min(essay_len), np.mean(essay_len), max(essay_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:34.831037Z",
     "iopub.status.busy": "2022-06-19T13:18:34.830651Z",
     "iopub.status.idle": "2022-06-19T13:18:35.054199Z",
     "shell.execute_reply": "2022-06-19T13:18:35.053455Z",
     "shell.execute_reply.started": "2022-06-19T13:18:34.831002Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(essay_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shortest essay is 691 words long, the longest is 11,641 words long, with a mean of 2315 words. The distribution is long-tailed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at the `discourse_text`, which is what we are trying to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:35.055699Z",
     "iopub.status.busy": "2022-06-19T13:18:35.055453Z",
     "iopub.status.idle": "2022-06-19T13:18:35.071149Z",
     "shell.execute_reply": "2022-06-19T13:18:35.070319Z",
     "shell.execute_reply.started": "2022-06-19T13:18:35.055665Z"
    }
   },
   "outputs": [],
   "source": [
    "text_len = [len(x) for x in df.discourse_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:35.07303Z",
     "iopub.status.busy": "2022-06-19T13:18:35.072379Z",
     "iopub.status.idle": "2022-06-19T13:18:35.397033Z",
     "shell.execute_reply": "2022-06-19T13:18:35.396383Z",
     "shell.execute_reply.started": "2022-06-19T13:18:35.07299Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:35.398407Z",
     "iopub.status.busy": "2022-06-19T13:18:35.39816Z",
     "iopub.status.idle": "2022-06-19T13:18:35.409489Z",
     "shell.execute_reply": "2022-06-19T13:18:35.408691Z",
     "shell.execute_reply.started": "2022-06-19T13:18:35.398373Z"
    }
   },
   "outputs": [],
   "source": [
    "min(text_len), np.mean(text_len), max(text_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the text has a minimum of 4 words, an average of ~250 words, but has a long-tailed distribution, with the maximum number of words being 4099. Once again, it seems like maybe there are some full essays that are being given a single annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `discourse_type` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:35.411351Z",
     "iopub.status.busy": "2022-06-19T13:18:35.411044Z",
     "iopub.status.idle": "2022-06-19T13:18:35.425115Z",
     "shell.execute_reply": "2022-06-19T13:18:35.424449Z",
     "shell.execute_reply.started": "2022-06-19T13:18:35.411316Z"
    }
   },
   "outputs": [],
   "source": [
    "df.discourse_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 7 types of discourse that are being annotated here. The data description for the competition provides more details:\n",
    "\n",
    "> Lead - an introduction that begins with a statistic, a quotation, a description, or some other device to grab the readerâ€™s attention and point toward the thesis\n",
    ">\n",
    "> Position - an opinion or conclusion on the main question\n",
    ">\n",
    "> Claim - a claim that supports the position\n",
    ">\n",
    "> Counterclaim - a claim that refutes another claim or gives an opposing reason to the position\n",
    ">\n",
    "> Rebuttal - a claim that refutes a counterclaim\n",
    ">\n",
    "> Evidence - ideas or examples that support claims, counterclaims, or rebuttals.\n",
    ">\n",
    "> Concluding Statement - a concluding statement that restates the claims\n",
    "\n",
    "\"Evidence\" is the most common type of discourse that is being annotated in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at our target variable that we need to predict, `discourse_effectiveness`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:35.426393Z",
     "iopub.status.busy": "2022-06-19T13:18:35.426152Z",
     "iopub.status.idle": "2022-06-19T13:18:35.439662Z",
     "shell.execute_reply": "2022-06-19T13:18:35.43892Z",
     "shell.execute_reply.started": "2022-06-19T13:18:35.426361Z"
    }
   },
   "outputs": [],
   "source": [
    "df.discourse_effectiveness.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three options: Ineffective, Effective, and Adequate. The data description provides more info:\n",
    "\n",
    ">Human readers rated each rhetorical or argumentative element, in order of increasing quality, as one of:\n",
    "> * Ineffective\n",
    "> * Adequate\n",
    "> * Effective\n",
    "\n",
    "More details on the annotation scheme is provided [here](https://docs.google.com/document/d/1G51Ulb0i-nKCRQSs4p4ujauy4wjAJOae)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:35.441342Z",
     "iopub.status.busy": "2022-06-19T13:18:35.440996Z",
     "iopub.status.idle": "2022-06-19T13:18:35.461116Z",
     "shell.execute_reply": "2022-06-19T13:18:35.460445Z",
     "shell.execute_reply.started": "2022-06-19T13:18:35.441307Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df.discourse_effectiveness == 'Effective'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:35.462327Z",
     "iopub.status.busy": "2022-06-19T13:18:35.462094Z",
     "iopub.status.idle": "2022-06-19T13:18:35.479702Z",
     "shell.execute_reply": "2022-06-19T13:18:35.479058Z",
     "shell.execute_reply.started": "2022-06-19T13:18:35.462295Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df.discourse_effectiveness == 'Ineffective'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have looked at all the columns from the training CSV. Remember that `discourse_effectiveness` is not in the test CSV since that is what we are predicting but all the other columns are in there. \n",
    "\n",
    "Finally let's look at the `sample_submission.csv` to understand what we need to submit and what our metric is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:35.481203Z",
     "iopub.status.busy": "2022-06-19T13:18:35.480835Z",
     "iopub.status.idle": "2022-06-19T13:18:35.497264Z",
     "shell.execute_reply": "2022-06-19T13:18:35.496606Z",
     "shell.execute_reply.started": "2022-06-19T13:18:35.48117Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(path/'sample_submission.csv')\n",
    "sample_df.head()"
   ]
  },
  {
   "attachments": {
    "6ba3da99-a3bf-4262-9c33-b4b65a6ac1ba.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We effectively just need to submit the logits from our model for each of the classes. These logits are rescaled to sum up to 1 (so like a softmax) by Kaggle and the metric is calculated. The metric used is a log loss:\n",
    "![image.png](attachment:6ba3da99-a3bf-4262-9c33-b4b65a6ac1ba.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll put together a simple baseline that completely ignores the full essay and only uses the discourse text to make a classification.\n",
    "\n",
    "Let's import the stuff necessary for training a model. We will be using HuggingFace Transformers to train our model (specifically it's `Trainer` API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:35.498834Z",
     "iopub.status.busy": "2022-06-19T13:18:35.498409Z",
     "iopub.status.idle": "2022-06-19T13:18:43.299205Z",
     "shell.execute_reply": "2022-06-19T13:18:43.298458Z",
     "shell.execute_reply.started": "2022-06-19T13:18:35.498796Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import warnings,transformers,logging,torch\n",
    "from transformers import TrainingArguments,Trainer\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:43.305768Z",
     "iopub.status.busy": "2022-06-19T13:18:43.303489Z",
     "iopub.status.idle": "2022-06-19T13:18:53.630065Z",
     "shell.execute_reply": "2022-06-19T13:18:53.629149Z",
     "shell.execute_reply.started": "2022-06-19T13:18:43.305722Z"
    }
   },
   "outputs": [],
   "source": [
    "if iskaggle:\n",
    "    !pip install --no-index --find-links ../input/huggingface-datasets datasets -q\n",
    "import datasets\n",
    "from datasets import load_dataset, Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiet down some of the warnings produced by HuggingFace Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:53.633044Z",
     "iopub.status.busy": "2022-06-19T13:18:53.632743Z",
     "iopub.status.idle": "2022-06-19T13:18:53.638486Z",
     "shell.execute_reply": "2022-06-19T13:18:53.637422Z",
     "shell.execute_reply.started": "2022-06-19T13:18:53.633002Z"
    }
   },
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeBERTA v3 Large is a small and simple model good for experimentation. Since we want to submit this notebook, the model is loaded as a dataset so the notebook can stay offline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:53.640336Z",
     "iopub.status.busy": "2022-06-19T13:18:53.63995Z",
     "iopub.status.idle": "2022-06-19T13:18:53.64875Z",
     "shell.execute_reply": "2022-06-19T13:18:53.647981Z",
     "shell.execute_reply.started": "2022-06-19T13:18:53.640297Z"
    }
   },
   "outputs": [],
   "source": [
    "model_nm = '../input/debertav3small'\n",
    "#model_nm = '../input/deberta-v3-base/deberta-v3-base'\n",
    "#model_nm = '../input/deberta-v3-large'\n",
    "#model_nm='../input/deberta-v3-large/deberta-v3-large'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get the tokenizer for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:53.650276Z",
     "iopub.status.busy": "2022-06-19T13:18:53.649773Z",
     "iopub.status.idle": "2022-06-19T13:18:54.350513Z",
     "shell.execute_reply": "2022-06-19T13:18:54.349758Z",
     "shell.execute_reply.started": "2022-06-19T13:18:53.650232Z"
    }
   },
   "outputs": [],
   "source": [
    "tokz = AutoTokenizer.from_pretrained(model_nm)\n",
    "#tokz = AutoTokenizer.from_pretrained('microsoft/deberta-v3-large')#, model_max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our baseline, we will concatenate the discourse type and the discourse text and pass to our model. We need to separate the discourse type and the discourse text so that our model knows which is which. We will use the special separator token that the tokenizer has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:54.352027Z",
     "iopub.status.busy": "2022-06-19T13:18:54.351776Z",
     "iopub.status.idle": "2022-06-19T13:18:54.359175Z",
     "shell.execute_reply": "2022-06-19T13:18:54.358531Z",
     "shell.execute_reply.started": "2022-06-19T13:18:54.351988Z"
    }
   },
   "outputs": [],
   "source": [
    "sep = tokz.sep_token\n",
    "sep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:54.361355Z",
     "iopub.status.busy": "2022-06-19T13:18:54.360723Z",
     "iopub.status.idle": "2022-06-19T13:18:54.385941Z",
     "shell.execute_reply": "2022-06-19T13:18:54.385268Z",
     "shell.execute_reply.started": "2022-06-19T13:18:54.36131Z"
    }
   },
   "outputs": [],
   "source": [
    "df['inputs'] = df.discourse_type + sep +df.discourse_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace expects that the target is in a column to be called `label`, and also that the targets are numerical. We will categorize it and create a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:54.387517Z",
     "iopub.status.busy": "2022-06-19T13:18:54.387281Z",
     "iopub.status.idle": "2022-06-19T13:18:54.430932Z",
     "shell.execute_reply": "2022-06-19T13:18:54.430228Z",
     "shell.execute_reply.started": "2022-06-19T13:18:54.387485Z"
    }
   },
   "outputs": [],
   "source": [
    "new_label = {\"discourse_effectiveness\": {\"Ineffective\": 0, \"Adequate\": 1, \"Effective\": 2}}\n",
    "df = df.replace(new_label)\n",
    "df = df.rename(columns = {\"discourse_effectiveness\": \"label\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our `Dataset` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:54.432632Z",
     "iopub.status.busy": "2022-06-19T13:18:54.432356Z",
     "iopub.status.idle": "2022-06-19T13:18:54.489176Z",
     "shell.execute_reply": "2022-06-19T13:18:54.488479Z",
     "shell.execute_reply.started": "2022-06-19T13:18:54.432596Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tokenize the data, let's create a function, since that's what `Dataset.map` will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:54.492209Z",
     "iopub.status.busy": "2022-06-19T13:18:54.491716Z",
     "iopub.status.idle": "2022-06-19T13:18:54.496568Z",
     "shell.execute_reply": "2022-06-19T13:18:54.495792Z",
     "shell.execute_reply.started": "2022-06-19T13:18:54.492167Z"
    }
   },
   "outputs": [],
   "source": [
    "def tok_func(x): return tokz(x[\"inputs\"], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what one example looks like when tokenized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:54.498283Z",
     "iopub.status.busy": "2022-06-19T13:18:54.497921Z",
     "iopub.status.idle": "2022-06-19T13:18:54.509215Z",
     "shell.execute_reply": "2022-06-19T13:18:54.508346Z",
     "shell.execute_reply.started": "2022-06-19T13:18:54.498246Z"
    }
   },
   "outputs": [],
   "source": [
    "tok_func(ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now tokenize the  the input. We'll use `Dataset.map` to speed it up, and remove the columns we no longer need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:18:54.511163Z",
     "iopub.status.busy": "2022-06-19T13:18:54.510764Z",
     "iopub.status.idle": "2022-06-19T13:19:12.889031Z",
     "shell.execute_reply": "2022-06-19T13:19:12.888142Z",
     "shell.execute_reply.started": "2022-06-19T13:18:54.511125Z"
    }
   },
   "outputs": [],
   "source": [
    "inps = \"discourse_text\",\"discourse_type\"\n",
    "tok_ds = ds.map(tok_func, batched=True, remove_columns=inps+('inputs','discourse_id','essay_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see all the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:19:12.891188Z",
     "iopub.status.busy": "2022-06-19T13:19:12.890771Z",
     "iopub.status.idle": "2022-06-19T13:19:12.899427Z",
     "shell.execute_reply": "2022-06-19T13:19:12.898553Z",
     "shell.execute_reply.started": "2022-06-19T13:19:12.89115Z"
    }
   },
   "outputs": [],
   "source": [
    "tok_ds[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to split the dataset into a training set and a validation set. We will split based on essays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:19:12.901207Z",
     "iopub.status.busy": "2022-06-19T13:19:12.900815Z",
     "iopub.status.idle": "2022-06-19T13:19:12.919379Z",
     "shell.execute_reply": "2022-06-19T13:19:12.918721Z",
     "shell.execute_reply.started": "2022-06-19T13:19:12.901169Z"
    }
   },
   "outputs": [],
   "source": [
    "essay_ids = df.essay_id.unique()\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(essay_ids)\n",
    "essay_ids[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do a random 80%-20% split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:19:12.925978Z",
     "iopub.status.busy": "2022-06-19T13:19:12.925787Z",
     "iopub.status.idle": "2022-06-19T13:19:12.93147Z",
     "shell.execute_reply": "2022-06-19T13:19:12.930603Z",
     "shell.execute_reply.started": "2022-06-19T13:19:12.925954Z"
    }
   },
   "outputs": [],
   "source": [
    "val_prop = 0.2\n",
    "val_sz = int(len(essay_ids)*val_prop)\n",
    "val_essay_ids = essay_ids[:val_sz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:19:12.93409Z",
     "iopub.status.busy": "2022-06-19T13:19:12.93287Z",
     "iopub.status.idle": "2022-06-19T13:19:14.341014Z",
     "shell.execute_reply": "2022-06-19T13:19:14.340173Z",
     "shell.execute_reply.started": "2022-06-19T13:19:12.934051Z"
    }
   },
   "outputs": [],
   "source": [
    "is_val = np.isin(df.essay_id, val_essay_ids)\n",
    "idxs = np.arange(len(df))\n",
    "val_idxs = idxs[ is_val]\n",
    "trn_idxs = idxs[~is_val]\n",
    "len(val_idxs),len(trn_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `select` method of the `Dataset` object to create our splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:19:14.342851Z",
     "iopub.status.busy": "2022-06-19T13:19:14.342579Z",
     "iopub.status.idle": "2022-06-19T13:19:14.364442Z",
     "shell.execute_reply": "2022-06-19T13:19:14.363788Z",
     "shell.execute_reply.started": "2022-06-19T13:19:14.342815Z"
    }
   },
   "outputs": [],
   "source": [
    "dds = DatasetDict({\"train\":tok_ds.select(trn_idxs),\n",
    "             \"test\": tok_ds.select(val_idxs)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I put all of this into a single function, along with some extra code to deal with the test set (no split necessary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:19:14.366147Z",
     "iopub.status.busy": "2022-06-19T13:19:14.365815Z",
     "iopub.status.idle": "2022-06-19T13:19:14.371862Z",
     "shell.execute_reply": "2022-06-19T13:19:14.370983Z",
     "shell.execute_reply.started": "2022-06-19T13:19:14.366108Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dds(df, train=True):\n",
    "    ds = Dataset.from_pandas(df)\n",
    "    to_remove = ['discourse_text','discourse_type','inputs','discourse_id','essay_id']\n",
    "    tok_ds = ds.map(tok_func, batched=True, remove_columns=to_remove)\n",
    "    if train:\n",
    "        return DatasetDict({\"train\":tok_ds.select(trn_idxs), \"test\": tok_ds.select(val_idxs)})\n",
    "    else: \n",
    "        return tok_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train! Let's set some hyperparameters. We select a reasonable LR and a batch size that fits in the GPU RAM. I only train for 1 epoch here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:19:14.373865Z",
     "iopub.status.busy": "2022-06-19T13:19:14.373175Z",
     "iopub.status.idle": "2022-06-19T13:19:14.381156Z",
     "shell.execute_reply": "2022-06-19T13:19:14.380293Z",
     "shell.execute_reply.started": "2022-06-19T13:19:14.373823Z"
    }
   },
   "outputs": [],
   "source": [
    "lr,bs = 0.0001,16\n",
    "wd,epochs = 0.01,3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our metric. Note that technically our metric is the same as our loss function, but I include it here for clarity and teaching purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:19:14.382994Z",
     "iopub.status.busy": "2022-06-19T13:19:14.382704Z",
     "iopub.status.idle": "2022-06-19T13:19:14.390316Z",
     "shell.execute_reply": "2022-06-19T13:19:14.389636Z",
     "shell.execute_reply.started": "2022-06-19T13:19:14.382947Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "import torch.nn.functional as F\n",
    "def score(preds): return {'log loss': log_loss(preds.label_ids, F.softmax(torch.Tensor(preds.predictions)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our model and trainer. HuggingFace uses the `TrainingArguments` class to set up arguments. We'll use a cosine scheduler with warmup. We'll use fp16 since it's much faster on modern GPUs, and saves some memory. We evaluate using double-sized batches, since no gradients are stored so we can do twice as many rows at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:19:14.39202Z",
     "iopub.status.busy": "2022-06-19T13:19:14.391732Z",
     "iopub.status.idle": "2022-06-19T13:19:14.399947Z",
     "shell.execute_reply": "2022-06-19T13:19:14.39932Z",
     "shell.execute_reply.started": "2022-06-19T13:19:14.391985Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_trainer(dds):\n",
    "    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
    "        evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "        num_train_epochs=epochs, weight_decay=wd, report_to='none')\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=3)\n",
    "    return Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "                   tokenizer=tokz, compute_metrics=score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:19:14.402525Z",
     "iopub.status.busy": "2022-06-19T13:19:14.402125Z",
     "iopub.status.idle": "2022-06-19T13:44:23.312959Z",
     "shell.execute_reply": "2022-06-19T13:44:23.31214Z",
     "shell.execute_reply.started": "2022-06-19T13:19:14.402472Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = get_trainer(dds)\n",
    "if hasattr(torch.cuda, 'empty_cache'):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get our test CSV again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:44:23.314632Z",
     "iopub.status.busy": "2022-06-19T13:44:23.314303Z",
     "iopub.status.idle": "2022-06-19T13:44:23.328437Z",
     "shell.execute_reply": "2022-06-19T13:44:23.327809Z",
     "shell.execute_reply.started": "2022-06-19T13:44:23.314595Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(path/'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and process the same way we did with the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:44:23.329931Z",
     "iopub.status.busy": "2022-06-19T13:44:23.329614Z",
     "iopub.status.idle": "2022-06-19T13:44:23.335891Z",
     "shell.execute_reply": "2022-06-19T13:44:23.335143Z",
     "shell.execute_reply.started": "2022-06-19T13:44:23.329894Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df['inputs'] = test_df.discourse_type + sep + test_df.discourse_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:44:23.337759Z",
     "iopub.status.busy": "2022-06-19T13:44:23.337404Z",
     "iopub.status.idle": "2022-06-19T13:44:24.41225Z",
     "shell.execute_reply": "2022-06-19T13:44:24.411599Z",
     "shell.execute_reply.started": "2022-06-19T13:44:23.337722Z"
    }
   },
   "outputs": [],
   "source": [
    "test_ds = get_dds(test_df,train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our `Dataset` object with our test dataset. Then we can simply perform `Trainer.predict` on our dataset to get the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:44:24.414154Z",
     "iopub.status.busy": "2022-06-19T13:44:24.413684Z",
     "iopub.status.idle": "2022-06-19T13:44:24.456909Z",
     "shell.execute_reply": "2022-06-19T13:44:24.456053Z",
     "shell.execute_reply.started": "2022-06-19T13:44:24.414116Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = F.softmax(torch.Tensor(trainer.predict(test_ds).predictions)).numpy().astype(float)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put it in a CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:44:24.458583Z",
     "iopub.status.busy": "2022-06-19T13:44:24.458227Z",
     "iopub.status.idle": "2022-06-19T13:44:24.48088Z",
     "shell.execute_reply": "2022-06-19T13:44:24.480297Z",
     "shell.execute_reply.started": "2022-06-19T13:44:24.458528Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv(path/'sample_submission.csv')\n",
    "submission_df['Ineffective'] = preds[:,0]\n",
    "submission_df['Adequate'] = preds[:,1]\n",
    "submission_df['Effective'] = preds[:,2]\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T13:44:24.482086Z",
     "iopub.status.busy": "2022-06-19T13:44:24.481767Z",
     "iopub.status.idle": "2022-06-19T13:44:24.495355Z",
     "shell.execute_reply": "2022-06-19T13:44:24.494522Z",
     "shell.execute_reply.started": "2022-06-19T13:44:24.482056Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we are good to go! It's as simple as that! ðŸ™‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to try:\n",
    "\n",
    "Try different ways of processing and creating our input:\n",
    "- Include the full essay data (might be tough for some models with limited context length)\n",
    "- Try different separators, order of dataset\n",
    "\n",
    "Try different models:\n",
    "- Bigger models\n",
    "- Other BERT-like models\n",
    "- Are there any pretrained models relevant to this dataset available?\n",
    "\n",
    "Use previous competition data and insights (https://www.kaggle.com/competitions/feedback-prize-2021/)\n",
    "\n",
    "Hope this helps you get started with this competition!\n",
    "\n",
    "NOTE: Thie notebook was heavily inspired by Jeremy Howard's [amazing](https://www.kaggle.com/code/jhoward/iterate-like-a-grandmaster) [notebooks](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners). Be sure to check them out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**If you found this notebook helpful, please give it an upvote ðŸ™‚**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
