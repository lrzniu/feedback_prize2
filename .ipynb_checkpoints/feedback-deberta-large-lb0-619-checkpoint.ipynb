{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The below inference code is my current submission code(2022-06-27). As I continuously learn from the community, I've gained much knowledge about model building and tuning. I want to share my current progress and will keep moving forward and welcome new insights from the community.  \n",
    "This submission code combines the great work from below links. I really learned a lot from the masters.  \n",
    "1. [PPPM / Deberta-v3-large baseline w/ W&B [train]](https://www.kaggle.com/code/yasufuminakama/pppm-deberta-v3-large-baseline-w-w-b-train)  \n",
    "2. [Optimization approaches for Transformers](https://www.kaggle.com/code/vad13irt/optimization-approaches-for-transformers/notebook)  \n",
    "3. [4th place solution of 2021-feedback](https://www.kaggle.com/competitions/feedback-prize-2021/discussion/313330#1724876)  \n",
    "\n",
    "## So far useful attempts  \n",
    "* 512 max length\n",
    "* WeightedLayerPooling (Slightly improve deberta-base model from simple \\[CLS] head) \n",
    "* GroupFold \n",
    "* Different learning rates across layers\n",
    "* Preprocessing (encoding-resolve+normalize) \n",
    "\n",
    "## About to try\n",
    "* Currently doing more EDA, trying to find out more potential gaining point from the text data instead of just tuning hyper-parameters :)  \n",
    "* Fix my deberta-v3 models :(\n",
    "* Working on 1024 max length.\n",
    "\n",
    "## CV / LB  \n",
    "* deberta-base   CV:0.662 / LB: 0.631\n",
    "* deberta-large  CV:0.637 / LB: 0.619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-21T03:22:50.33875Z",
     "iopub.status.busy": "2022-06-21T03:22:50.336264Z",
     "iopub.status.idle": "2022-06-21T03:22:58.802905Z",
     "shell.execute_reply": "2022-06-21T03:22:58.801927Z",
     "shell.execute_reply.started": "2022-06-21T03:22:50.338621Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import shutil\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import datasets, transformers\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import shutil\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "print(f\"torch.__version__: {torch.__version__}\")\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# os.system('pip uninstall -y transformers')\n",
    "# os.system('pip uninstall -y tokenizers')\n",
    "# os.system('python -m pip install --no-index --find-links=../input/pppm-pip-wheels transformers')\n",
    "# os.system('python -m pip install --no-index --find-links=../input/pppm-pip-wheels tokenizers')\n",
    "import tokenizers\n",
    "import transformers\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T03:22:58.805069Z",
     "iopub.status.busy": "2022-06-21T03:22:58.804593Z",
     "iopub.status.idle": "2022-06-21T03:22:58.813654Z",
     "shell.execute_reply": "2022-06-21T03:22:58.812918Z",
     "shell.execute_reply.started": "2022-06-21T03:22:58.805043Z"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    num_workers=1\n",
    "    path=\"../input/feedback-deberta-large-051/\"\n",
    "    config_path=path+'config.pth'\n",
    "    model=\"microsoft/deberta-large\"\n",
    "    batch_size=16\n",
    "    fc_dropout=0.2\n",
    "    target_size=3\n",
    "    max_len=512\n",
    "    seed=42\n",
    "    n_fold=4\n",
    "    trn_fold=[i for i in range(n_fold)]\n",
    "    gradient_checkpoint=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T03:22:58.815612Z",
     "iopub.status.busy": "2022-06-21T03:22:58.81516Z",
     "iopub.status.idle": "2022-06-21T03:22:58.823895Z",
     "shell.execute_reply": "2022-06-21T03:22:58.822989Z",
     "shell.execute_reply.started": "2022-06-21T03:22:58.815577Z"
    }
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "\n",
    "def get_essay(essay_id, is_train=True):\n",
    "    parent_path = INPUT_DIR + 'train' if is_train else INPUT_DIR + 'test'\n",
    "    essay_path = os.path.join(parent_path, f\"{essay_id}.txt\")\n",
    "    essay_text = open(essay_path, 'r').read()\n",
    "    return essay_text\n",
    "\n",
    "def softmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    y_pred = softmax(y_pred)\n",
    "    score = log_loss(y_true, y_pred)\n",
    "    return round(score, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T03:22:58.826576Z",
     "iopub.status.busy": "2022-06-21T03:22:58.826195Z",
     "iopub.status.idle": "2022-06-21T03:22:58.858871Z",
     "shell.execute_reply": "2022-06-21T03:22:58.858156Z",
     "shell.execute_reply.started": "2022-06-21T03:22:58.82653Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = \"feedback-prize-effectiveness/\"\n",
    "test = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))\n",
    "submission = pd.read_csv(os.path.join(INPUT_DIR, 'sample_submission.csv'))\n",
    "test['essay_text']  = test['essay_id'].apply(lambda x: get_essay(x, is_train=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T03:22:58.860588Z",
     "iopub.status.busy": "2022-06-21T03:22:58.860185Z",
     "iopub.status.idle": "2022-06-21T03:22:58.995296Z",
     "shell.execute_reply": "2022-06-21T03:22:58.994498Z",
     "shell.execute_reply.started": "2022-06-21T03:22:58.860553Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# tokenizer\n",
    "# ====================================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.path + 'tokenizer')\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_unidecode import unidecode\n",
    "from typing import Dict, List, Tuple\n",
    "import codecs\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['discourse_text'] = test['discourse_text'].apply(lambda x : resolve_encodings_and_normalize(x))\n",
    "test['essay_text'] = test['essay_text'].apply(lambda x : resolve_encodings_and_normalize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T03:22:58.996745Z",
     "iopub.status.busy": "2022-06-21T03:22:58.996395Z",
     "iopub.status.idle": "2022-06-21T03:22:59.019417Z",
     "shell.execute_reply": "2022-06-21T03:22:59.018601Z",
     "shell.execute_reply.started": "2022-06-21T03:22:58.996712Z"
    }
   },
   "outputs": [],
   "source": [
    "SEP = tokenizer.sep_token\n",
    "test['text'] = test['discourse_type'] + ' ' + test['discourse_text'] + SEP + test['essay_text']\n",
    "test['label'] = np.nan\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T03:28:01.138104Z",
     "iopub.status.busy": "2022-06-21T03:28:01.137703Z",
     "iopub.status.idle": "2022-06-21T03:28:01.146957Z",
     "shell.execute_reply": "2022-06-21T03:28:01.14557Z",
     "shell.execute_reply.started": "2022-06-21T03:28:01.138065Z"
    }
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.text = df['text'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = self.cfg.tokenizer.encode_plus(\n",
    "                        self.text[item],\n",
    "                        truncation=True,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=self.cfg.max_len\n",
    "                    )\n",
    "        samples = {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "        }\n",
    "\n",
    "        if 'token_type_ids' in inputs:\n",
    "            samples['token_type_ids'] = inputs['token_type_ids']\n",
    "        \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T03:28:36.245661Z",
     "iopub.status.busy": "2022-06-21T03:28:36.244616Z",
     "iopub.status.idle": "2022-06-21T03:28:36.261578Z",
     "shell.execute_reply": "2022-06-21T03:28:36.260615Z",
     "shell.execute_reply.started": "2022-06-21T03:28:36.245621Z"
    }
   },
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    def __init__(self, tokenizer, isTrain=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.isTrain = isTrain\n",
    "        # self.args = args\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n",
    "        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = [sample[\"target\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n",
    "        else:\n",
    "            output[\"input_ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"attention_mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n",
    "        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = torch.tensor(output[\"target\"], dtype=torch.long)\n",
    "\n",
    "        return output\n",
    "\n",
    "collate_fn = Collate(CFG.tokenizer, isTrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T03:22:59.048608Z",
     "iopub.status.busy": "2022-06-21T03:22:59.047901Z",
     "iopub.status.idle": "2022-06-21T03:22:59.056818Z",
     "shell.execute_reply": "2022-06-21T03:22:59.055964Z",
     "shell.execute_reply.started": "2022-06-21T03:22:59.048572Z"
    }
   },
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "    \n",
    "class MeanMaxPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanMaxPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        mean_pooling_embeddings = torch.mean(last_hidden_state, 1)\n",
    "        _, max_pooling_embeddings = torch.max(last_hidden_state, 1)\n",
    "        mean_max_embeddings = torch.cat((mean_pooling_embeddings, max_pooling_embeddings), 1)\n",
    "        return mean_max_embeddings\n",
    "\n",
    "    \n",
    "class LSTMPooling(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, hiddendim_lstm):\n",
    "        super(LSTMPooling, self).__init__()\n",
    "        self.num_hidden_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hiddendim_lstm = hiddendim_lstm\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hiddendim_lstm, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, all_hidden_states):\n",
    "        ## forward\n",
    "        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n",
    "                                     for layer_i in range(1, self.num_hidden_layers+1)], dim=-1)\n",
    "        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n",
    "        out, _ = self.lstm(hidden_states, None)\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n",
    "            )\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        return weighted_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T03:22:59.061876Z",
     "iopub.status.busy": "2022-06-21T03:22:59.061377Z",
     "iopub.status.idle": "2022-06-21T03:22:59.080417Z",
     "shell.execute_reply": "2022-06-21T03:22:59.079505Z",
     "shell.execute_reply.started": "2022-06-21T03:22:59.061848Z"
    }
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "from torch.cuda.amp import autocast\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        \n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel.from_config(self.config)\n",
    "        \n",
    "        # gradient checkpointing  梯度检查点\n",
    "        if self.cfg.gradient_checkpoint:\n",
    "            self.model.gradient_checkpointing_enable()  \n",
    "            print(f\"Gradient Checkpointing: {self.model.is_gradient_checkpointing}\")\n",
    "            \n",
    "        \n",
    "        # self.pooler = MeanPooling()\n",
    "        \n",
    "        self.bilstm = nn.LSTM(self.config.hidden_size, (self.config.hidden_size) // 2, num_layers=2, \n",
    "                              dropout=self.config.hidden_dropout_prob, batch_first=True,\n",
    "                              bidirectional=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(self.config.hidden_size, self.cfg.target_size)\n",
    "            # nn.Linear(256, self.cfg.target_size)\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "    def loss(self, outputs, targets):\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(outputs, targets)\n",
    "        return loss\n",
    "    \n",
    "    def monitor_metrics(self, outputs, targets):\n",
    "        device = targets.get_device()\n",
    "        # print(outputs)\n",
    "        # print(targets)\n",
    "        mll = log_loss(\n",
    "            targets.cpu().detach().numpy(),\n",
    "            softmax(outputs.cpu().detach().numpy()),\n",
    "            labels=[0, 1, 2],\n",
    "        )\n",
    "        return mll\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids=None, targets=None):\n",
    "        if token_type_ids:\n",
    "            transformer_out = self.model(ids, mask, token_type_ids)\n",
    "        else:\n",
    "            transformer_out = self.model(ids, mask)\n",
    "        \n",
    "        # LSTM/GRU header\n",
    "#         all_hidden_states = torch.stack(transformer_out[1])\n",
    "#         sequence_output = self.pooler(all_hidden_states)\n",
    "        \n",
    "        # simple CLS\n",
    "        sequence_output = transformer_out[0][:, 0, :]\n",
    "\n",
    "        \n",
    "        # Main task\n",
    "        logits1 = self.output(self.dropout1(sequence_output))\n",
    "        logits2 = self.output(self.dropout2(sequence_output))\n",
    "        logits3 = self.output(self.dropout3(sequence_output))\n",
    "        logits4 = self.output(self.dropout4(sequence_output))\n",
    "        logits5 = self.output(self.dropout5(sequence_output))\n",
    "        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n",
    "\n",
    "        if targets is not None:\n",
    "            metric = self.monitor_metrics(logits, targets)\n",
    "            return logits, metric\n",
    "        \n",
    "        return logits, 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T03:34:00.229265Z",
     "iopub.status.busy": "2022-06-21T03:34:00.22824Z",
     "iopub.status.idle": "2022-06-21T03:34:00.237602Z",
     "shell.execute_reply": "2022-06-21T03:34:00.236736Z",
     "shell.execute_reply.started": "2022-06-21T03:34:00.229225Z"
    }
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# inference\n",
    "# ====================================================\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for data in tk0:\n",
    "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        with torch.no_grad():\n",
    "            y_preds, _ = model(ids, mask)\n",
    "        y_preds = softmax(y_preds.to('cpu').numpy())\n",
    "        preds.append(y_preds)\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T03:28:40.261549Z",
     "iopub.status.busy": "2022-06-21T03:28:40.261188Z",
     "iopub.status.idle": "2022-06-21T03:28:40.266928Z",
     "shell.execute_reply": "2022-06-21T03:28:40.265858Z",
     "shell.execute_reply.started": "2022-06-21T03:28:40.261518Z"
    }
   },
   "outputs": [],
   "source": [
    "deberta_predictions = []\n",
    "test_dataset = TestDataset(CFG, test)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=CFG.batch_size,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=collate_fn,\n",
    "                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T03:34:38.970807Z",
     "iopub.status.busy": "2022-06-21T03:34:38.970393Z",
     "iopub.status.idle": "2022-06-21T03:36:17.882784Z",
     "shell.execute_reply": "2022-06-21T03:36:17.881805Z",
     "shell.execute_reply.started": "2022-06-21T03:34:38.970767Z"
    }
   },
   "outputs": [],
   "source": [
    "deberta_predictions = []\n",
    "for fold in CFG.trn_fold:\n",
    "    print(\"Fold {}\".format(fold))\n",
    "\n",
    "    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n",
    "    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "                       map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state['model'])\n",
    "    prediction = inference_fn(test_loader, model, device)\n",
    "    deberta_predictions.append(prediction)\n",
    "    del model, state, prediction; gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T03:37:15.189291Z",
     "iopub.status.busy": "2022-06-21T03:37:15.188939Z",
     "iopub.status.idle": "2022-06-21T03:37:15.193857Z",
     "shell.execute_reply": "2022-06-21T03:37:15.192628Z",
     "shell.execute_reply.started": "2022-06-21T03:37:15.189262Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = np.mean(deberta_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T03:39:12.204887Z",
     "iopub.status.busy": "2022-06-21T03:39:12.204549Z",
     "iopub.status.idle": "2022-06-21T03:39:12.221975Z",
     "shell.execute_reply": "2022-06-21T03:39:12.220458Z",
     "shell.execute_reply.started": "2022-06-21T03:39:12.204859Z"
    }
   },
   "outputs": [],
   "source": [
    "submission['Ineffective'] = predictions[:, 0]\n",
    "submission['Adequate'] = predictions[:, 1]\n",
    "submission['Effective'] = predictions[:, 2]\n",
    "\n",
    "display(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T03:40:06.899651Z",
     "iopub.status.busy": "2022-06-21T03:40:06.898912Z",
     "iopub.status.idle": "2022-06-21T03:40:06.909199Z",
     "shell.execute_reply": "2022-06-21T03:40:06.90829Z",
     "shell.execute_reply.started": "2022-06-21T03:40:06.899615Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_transformer]",
   "language": "python",
   "name": "conda-env-torch_transformer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
