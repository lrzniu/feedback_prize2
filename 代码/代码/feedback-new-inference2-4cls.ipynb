{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bcd7eb5",
   "metadata": {
    "papermill": {
     "duration": 0.007407,
     "end_time": "2022-07-09T13:35:46.714027",
     "exception": false,
     "start_time": "2022-07-09T13:35:46.706620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3>ðŸ“Œ Train Notebook:</h3> <h4><a href='https://www.kaggle.com/code/debarshichanda/pytorch-feedback-deberta-v3-baseline'>https://www.kaggle.com/code/debarshichanda/pytorch-feedback-deberta-v3-baseline</a></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a762f04",
   "metadata": {
    "papermill": {
     "duration": 0.005893,
     "end_time": "2022-07-09T13:35:46.726205",
     "exception": false,
     "start_time": "2022-07-09T13:35:46.720312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* cv0.6819\n",
    "* lb:652\n",
    "* here code:https://www.kaggle.com/code/quincyqiang/feedback-meanpoolingv2-inference\n",
    "* thanks to:Debarshi Chanda\n",
    "* hitsï¼šBased on the baseline, trying the way of text splicing:add `discourse_type` and training on 5folds\n",
    "\n",
    "```\n",
    "text = discourse_type+self.tokenizer.sep_token+discourse +self.tokenizer.sep_token + \" \" + essay\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f202cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T13:35:46.741577Z",
     "iopub.status.busy": "2022-07-09T13:35:46.741002Z",
     "iopub.status.idle": "2022-07-09T13:35:53.644744Z",
     "shell.execute_reply": "2022-07-09T13:35:53.643769Z"
    },
    "papermill": {
     "duration": 6.914657,
     "end_time": "2022-07-09T13:35:53.647267",
     "exception": false,
     "start_time": "2022-07-09T13:35:46.732610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# For Transformer Models\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For descriptive error messages\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68ef7dad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T13:35:53.660787Z",
     "iopub.status.busy": "2022-07-09T13:35:53.660044Z",
     "iopub.status.idle": "2022-07-09T13:35:53.666048Z",
     "shell.execute_reply": "2022-07-09T13:35:53.665121Z"
    },
    "papermill": {
     "duration": 0.014789,
     "end_time": "2022-07-09T13:35:53.668085",
     "exception": false,
     "start_time": "2022-07-09T13:35:53.653296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_DIR='../input/com-base-lr2-e5-textchange-4cls'\n",
    "MODEL_PATHS = [\n",
    "    f'{MODEL_DIR}/Loss-Fold-0.bin',\n",
    "    f'{MODEL_DIR}/Loss-Fold-1.bin',\n",
    "    f'{MODEL_DIR}/Loss-Fold-2.bin',\n",
    "    f'{MODEL_DIR}/Loss-Fold-3.bin',\n",
    "    f'{MODEL_DIR}/Loss-Fold-4.bin',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70fbc194",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T13:35:53.681673Z",
     "iopub.status.busy": "2022-07-09T13:35:53.681343Z",
     "iopub.status.idle": "2022-07-09T13:35:53.685895Z",
     "shell.execute_reply": "2022-07-09T13:35:53.684953Z"
    },
    "papermill": {
     "duration": 0.013517,
     "end_time": "2022-07-09T13:35:53.688017",
     "exception": false,
     "start_time": "2022-07-09T13:35:53.674500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"../input/feedback-prize-effectiveness/train\"\n",
    "TEST_DIR = \"../input/feedback-prize-effectiveness/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bbf31cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T13:35:53.700772Z",
     "iopub.status.busy": "2022-07-09T13:35:53.700465Z",
     "iopub.status.idle": "2022-07-09T13:35:54.524886Z",
     "shell.execute_reply": "2022-07-09T13:35:54.523907Z"
    },
    "papermill": {
     "duration": 0.833403,
     "end_time": "2022-07-09T13:35:54.527175",
     "exception": false,
     "start_time": "2022-07-09T13:35:53.693772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "CONFIG = dict(\n",
    "    seed = 42,\n",
    "    model_name = '../input/deberta-v3-base/deberta-v3-base',\n",
    "    test_batch_size = 16,\n",
    "    max_length = 512,\n",
    "    num_classes = 3,\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    gradient_checkpoint=False\n",
    ")\n",
    "\n",
    "CONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eefa7053",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T13:35:54.540231Z",
     "iopub.status.busy": "2022-07-09T13:35:54.539904Z",
     "iopub.status.idle": "2022-07-09T13:35:54.551124Z",
     "shell.execute_reply": "2022-07-09T13:35:54.550153Z"
    },
    "papermill": {
     "duration": 0.020152,
     "end_time": "2022-07-09T13:35:54.553225",
     "exception": false,
     "start_time": "2022-07-09T13:35:54.533073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_essay(essay_id):\n",
    "    essay_path = os.path.join(TEST_DIR, f\"{essay_id}.txt\")\n",
    "    essay_text = open(essay_path, 'r').read()\n",
    "    return essay_text\n",
    "\n",
    "def softmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    y_pred = softmax(y_pred)\n",
    "    score = log_loss(y_true, y_pred)\n",
    "    return round(score, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e00881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T13:35:54.567160Z",
     "iopub.status.busy": "2022-07-09T13:35:54.566456Z",
     "iopub.status.idle": "2022-07-09T13:35:54.618227Z",
     "shell.execute_reply": "2022-07-09T13:35:54.617122Z"
    },
    "papermill": {
     "duration": 0.061851,
     "end_time": "2022-07-09T13:35:54.620830",
     "exception": false,
     "start_time": "2022-07-09T13:35:54.558979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>essay_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a261b6e14276</td>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>Making choices in life can be very difficult. ...</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Making choices in life can be very difficult. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5a88900e7dc1</td>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>Seeking multiple opinions can help a person ma...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Making choices in life can be very difficult. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9790d835736b</td>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>it can decrease stress levels</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Making choices in life can be very difficult. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75ce6d68b67b</td>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>a great chance to learn something new</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Making choices in life can be very difficult. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93578d946723</td>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>can be very helpful and beneficial.</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Making choices in life can be very difficult. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id      essay_id  \\\n",
       "0  a261b6e14276  D72CB1C11673   \n",
       "1  5a88900e7dc1  D72CB1C11673   \n",
       "2  9790d835736b  D72CB1C11673   \n",
       "3  75ce6d68b67b  D72CB1C11673   \n",
       "4  93578d946723  D72CB1C11673   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Making choices in life can be very difficult. ...           Lead   \n",
       "1  Seeking multiple opinions can help a person ma...       Position   \n",
       "2                     it can decrease stress levels           Claim   \n",
       "3             a great chance to learn something new           Claim   \n",
       "4               can be very helpful and beneficial.           Claim   \n",
       "\n",
       "                                          essay_text  \n",
       "0  Making choices in life can be very difficult. ...  \n",
       "1  Making choices in life can be very difficult. ...  \n",
       "2  Making choices in life can be very difficult. ...  \n",
       "3  Making choices in life can be very difficult. ...  \n",
       "4  Making choices in life can be very difficult. ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../input/feedback-prize-effectiveness/test.csv\")\n",
    "df['essay_text'] = df['essay_id'].apply(get_essay)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bace9bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T13:35:54.634657Z",
     "iopub.status.busy": "2022-07-09T13:35:54.634335Z",
     "iopub.status.idle": "2022-07-09T13:35:54.663896Z",
     "shell.execute_reply": "2022-07-09T13:35:54.662884Z"
    },
    "papermill": {
     "duration": 0.039424,
     "end_time": "2022-07-09T13:35:54.666428",
     "exception": false,
     "start_time": "2022-07-09T13:35:54.627004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from text_unidecode import unidecode\n",
    "from typing import Dict, List, Tuple\n",
    "import codecs\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "df['discourse_text'] = df['discourse_text'].apply(lambda x : resolve_encodings_and_normalize(x))\n",
    "df['essay_text'] = df['essay_text'].apply(lambda x : resolve_encodings_and_normalize(x))\n",
    "#encoder = LabelEncoder()\n",
    "#df['discourse_effectiveness'] = encoder.fit_transform(df['discourse_effectiveness'])\n",
    "\n",
    "# with open(\"le.pkl\", \"wb\") as fp:\n",
    "#     joblib.dump(encoder, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fb28dcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T13:35:54.681021Z",
     "iopub.status.busy": "2022-07-09T13:35:54.679516Z",
     "iopub.status.idle": "2022-07-09T13:35:55.229088Z",
     "shell.execute_reply": "2022-07-09T13:35:55.228068Z"
    },
    "papermill": {
     "duration": 0.558579,
     "end_time": "2022-07-09T13:35:55.231111",
     "exception": false,
     "start_time": "2022-07-09T13:35:54.672532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.1.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Adequate', 'Effective', 'Ineffective'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"../input/{MODEL_DIR}/le.pkl\", \"rb\") as fp:\n",
    "    encoder = joblib.load(fp)\n",
    "    \n",
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e1648ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T13:35:55.245448Z",
     "iopub.status.busy": "2022-07-09T13:35:55.245001Z",
     "iopub.status.idle": "2022-07-09T13:35:55.255528Z",
     "shell.execute_reply": "2022-07-09T13:35:55.254630Z"
    },
    "papermill": {
     "duration": 0.020187,
     "end_time": "2022-07-09T13:35:55.257509",
     "exception": false,
     "start_time": "2022-07-09T13:35:55.237322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class FeedBackDataset(Dataset):\n",
    "#     def __init__(self, df, tokenizer, max_length):\n",
    "#         self.df = df\n",
    "#         self.max_len = max_length\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.discourse = df['discourse_text'].values\n",
    "#         self.essay = df['essay_text'].values\n",
    "#         #self.targets = df['discourse_effectiveness'].values\n",
    "#         self.discourse_type = df['discourse_type'].values\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         discourse = self.discourse[index]\n",
    "#         discourse_type = self.discourse_type[index]\n",
    "#         essay = self.essay[index]\n",
    "#         text = discourse_type+self.tokenizer.sep_token+discourse +self.tokenizer.sep_token + \" \" + essay\n",
    "#         inputs = self.tokenizer.encode_plus(\n",
    "#             text,\n",
    "#             truncation=True,\n",
    "#             add_special_tokens=True,\n",
    "#             max_length=self.max_len,\n",
    "#             padding='max_length'\n",
    "#         )\n",
    "\n",
    "#         return {\n",
    "#             'input_ids': inputs['input_ids'],\n",
    "#             'attention_mask': inputs['attention_mask'],\n",
    "#             #'target': self.targets[index]\n",
    "#         }\n",
    "\n",
    "class FeedBackDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.df = df\n",
    "        self.max_len = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.discourse_type = df['discourse_type'].values\n",
    "        self.discourse = df['discourse_text'].values\n",
    "        self.essay = df['essay_text'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        discourse_type = self.discourse_type[index]\n",
    "        discourse = self.discourse[index]\n",
    "        \n",
    "        essay = self.essay[index]\n",
    "        text = discourse_type+self.tokenizer.sep_token+discourse +self.tokenizer.sep_token + \" \" + essay\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "                        text,\n",
    "                        truncation=True,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=self.max_len,\n",
    "                        padding='max_length'\n",
    "                    )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0726139",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T13:35:55.273013Z",
     "iopub.status.busy": "2022-07-09T13:35:55.271985Z",
     "iopub.status.idle": "2022-07-09T13:35:55.284138Z",
     "shell.execute_reply": "2022-07-09T13:35:55.283323Z"
    },
    "papermill": {
     "duration": 0.021839,
     "end_time": "2022-07-09T13:35:55.286243",
     "exception": false,
     "start_time": "2022-07-09T13:35:55.264404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    def __init__(self, tokenizer, isTrain=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.isTrain = isTrain\n",
    "        # self.args = args\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n",
    "        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = [sample[\"target\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n",
    "        else:\n",
    "            output[\"input_ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"attention_mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n",
    "        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = torch.tensor(output[\"target\"], dtype=torch.long)\n",
    "\n",
    "        return output\n",
    "\n",
    "collate_fn = Collate(CONFIG[\"tokenizer\"], isTrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d1fc2b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T13:35:55.299832Z",
     "iopub.status.busy": "2022-07-09T13:35:55.299530Z",
     "iopub.status.idle": "2022-07-09T13:35:55.305939Z",
     "shell.execute_reply": "2022-07-09T13:35:55.305100Z"
    },
    "papermill": {
     "duration": 0.015449,
     "end_time": "2022-07-09T13:35:55.307872",
     "exception": false,
     "start_time": "2022-07-09T13:35:55.292423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = FeedBackDataset(df, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n",
    "                         num_workers=2, shuffle=False,collate_fn=collate_fn, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f75cb66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T13:35:55.321786Z",
     "iopub.status.busy": "2022-07-09T13:35:55.321475Z",
     "iopub.status.idle": "2022-07-09T13:35:55.337682Z",
     "shell.execute_reply": "2022-07-09T13:35:55.336575Z"
    },
    "papermill": {
     "duration": 0.025789,
     "end_time": "2022-07-09T13:35:55.339812",
     "exception": false,
     "start_time": "2022-07-09T13:35:55.314023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "\n",
    "class MeanMaxPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanMaxPooling, self).__init__()\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        mean_pooling_embeddings = torch.mean(last_hidden_state, 1)\n",
    "        _, max_pooling_embeddings = torch.max(last_hidden_state, 1)\n",
    "        mean_max_embeddings = torch.cat((mean_pooling_embeddings, max_pooling_embeddings), 1)\n",
    "        return mean_max_embeddings\n",
    "\n",
    "\n",
    "class LSTMPooling(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, hiddendim_lstm):\n",
    "        super(LSTMPooling, self).__init__()\n",
    "        self.num_hidden_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hiddendim_lstm = hiddendim_lstm\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hiddendim_lstm, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        ## forward\n",
    "        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n",
    "                                     for layer_i in range(1, self.num_hidden_layers + 1)], dim=-1)\n",
    "        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n",
    "        out, _ = self.lstm(hidden_states, None)\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights=None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "            torch.tensor([1] * (num_hidden_layers + 1 - layer_start), dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        return weighted_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f5da72f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T13:35:55.355450Z",
     "iopub.status.busy": "2022-07-09T13:35:55.355028Z",
     "iopub.status.idle": "2022-07-09T13:35:55.375324Z",
     "shell.execute_reply": "2022-07-09T13:35:55.374342Z"
    },
    "papermill": {
     "duration": 0.030662,
     "end_time": "2022-07-09T13:35:55.377348",
     "exception": false,
     "start_time": "2022-07-09T13:35:55.346686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedBackModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(FeedBackModel, self).__init__()\n",
    "        self.cfg = CONFIG\n",
    "        self.config = AutoConfig.from_pretrained(model_name,output_hidden_states=True)\n",
    "        self.model = AutoModel.from_pretrained(model_name,config=self.config)\n",
    "        # gradient checkpointing  æ¢¯åº¦æ£€æŸ¥ç‚¹\n",
    "        if CONFIG['gradient_checkpoint']:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "            print(f\"Gradient Checkpointing: {self.model.is_gradient_checkpointing}\")\n",
    "        self.bilstm = nn.LSTM(self.config.hidden_size, (self.config.hidden_size) // 2, num_layers=2,\n",
    "                              dropout=self.config.hidden_dropout_prob, batch_first=True,\n",
    "                              bidirectional=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        #-----------------\n",
    "        #self.pooler=WeightedLayerPooling(self.config.num_hidden_layers)\n",
    "        #self.pooler=MeanPooling()\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(self.config.hidden_size*4, CONFIG['num_classes'])\n",
    "            # nn.Linear(256, self.cfg.target_size)\n",
    "        )\n",
    "\n",
    "    def loss(self, outputs, targets):\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(outputs, targets)\n",
    "        return loss\n",
    "\n",
    "    def monitor_metrics(self, outputs, targets):\n",
    "        device = targets.get_device()\n",
    "        # print(outputs)\n",
    "        # print(targets)\n",
    "        mll = log_loss(\n",
    "            targets.cpu().detach().numpy(),\n",
    "            softmax(outputs.cpu().detach().numpy()),\n",
    "            labels=[0, 1, 2],\n",
    "        )\n",
    "        return mll\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    # def forward(self, ids, mask):\n",
    "    #     out = self.model(input_ids=ids, attention_mask=mask,\n",
    "    #                      output_hidden_states=False)\n",
    "    #     out = self.pooler(out.last_hidden_state, mask)\n",
    "    #     out = self.drop(out)\n",
    "    #     outputs = self.fc(out)\n",
    "    #     return outputs\n",
    "    def forward(self, ids, mask, token_type_ids=None, targets=None):\n",
    "        if token_type_ids:\n",
    "            transformer_out = self.model(ids, mask, token_type_ids)\n",
    "        else:\n",
    "            transformer_out = self.model(ids, mask)\n",
    "\n",
    "        # LSTM/GRU header\n",
    "        # all_hidden_states = torch.stack(transformer_out[1])\n",
    "        # sequence_output = self.pooler(all_hidden_states)3\n",
    "        all_hidden_states = torch.stack(transformer_out[1])\n",
    "\n",
    "        concatenate_pooling = torch.cat(\n",
    "            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]), -1\n",
    "        )\n",
    "        sequence_output = concatenate_pooling[:, 0]\n",
    "        # simple CLS\n",
    "        #transformer_out = self.pooler(transformer_out.last_hidden_state,mask)\n",
    "        #sequence_output = transformer_out\n",
    "        #sequence_output = transformer_out[0][:, 0, :]\n",
    "        #all_hidden_states = torch.stack(transformer_out[1])\n",
    "        #sequence_output=self.pooler(all_hidden_states)\n",
    "\n",
    "        # Main task\n",
    "        logits1 = self.output(self.dropout1(sequence_output))\n",
    "        logits2 = self.output(self.dropout2(sequence_output))\n",
    "        logits3 = self.output(self.dropout3(sequence_output))\n",
    "        logits4 = self.output(self.dropout4(sequence_output))\n",
    "        logits5 = self.output(self.dropout5(sequence_output))\n",
    "        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n",
    "\n",
    "        if targets is not None:\n",
    "            metric = self.monitor_metrics(logits, targets)\n",
    "            return logits\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57a0babb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T13:35:55.391502Z",
     "iopub.status.busy": "2022-07-09T13:35:55.390924Z",
     "iopub.status.idle": "2022-07-09T13:35:55.399720Z",
     "shell.execute_reply": "2022-07-09T13:35:55.398790Z"
    },
    "papermill": {
     "duration": 0.018138,
     "end_time": "2022-07-09T13:35:55.401805",
     "exception": false,
     "start_time": "2022-07-09T13:35:55.383667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    PREDS = []\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        \n",
    "        outputs = model(ids, mask)\n",
    "        outputs = F.softmax(outputs, dim=1)\n",
    "        PREDS.append(outputs.cpu().detach().numpy()) \n",
    "    \n",
    "    PREDS = np.concatenate(PREDS)\n",
    "    gc.collect()\n",
    "    \n",
    "    return PREDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a28e294d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T13:35:55.416753Z",
     "iopub.status.busy": "2022-07-09T13:35:55.416422Z",
     "iopub.status.idle": "2022-07-09T13:35:55.423238Z",
     "shell.execute_reply": "2022-07-09T13:35:55.422152Z"
    },
    "papermill": {
     "duration": 0.016911,
     "end_time": "2022-07-09T13:35:55.425546",
     "exception": false,
     "start_time": "2022-07-09T13:35:55.408635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(model_paths, dataloader, device):\n",
    "    final_preds = []\n",
    "    for i, path in enumerate(model_paths):\n",
    "        model = FeedBackModel(CONFIG['model_name'])\n",
    "        model.to(CONFIG['device'])\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        \n",
    "        print(f\"Getting predictions for model {i+1}\")\n",
    "        preds = valid_fn(model, dataloader, device)\n",
    "        final_preds.append(preds)\n",
    "    \n",
    "    final_preds = np.array(final_preds)\n",
    "    final_preds = np.mean(final_preds, axis=0)\n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "056feedc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T13:35:55.439883Z",
     "iopub.status.busy": "2022-07-09T13:35:55.439078Z",
     "iopub.status.idle": "2022-07-09T13:37:01.670252Z",
     "shell.execute_reply": "2022-07-09T13:37:01.669004Z"
    },
    "papermill": {
     "duration": 66.241127,
     "end_time": "2022-07-09T13:37:01.672804",
     "exception": false,
     "start_time": "2022-07-09T13:35:55.431677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/deberta-v3-base/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifer.bias', 'mask_predictions.classifer.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions for model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.46s/it]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-base/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifer.bias', 'mask_predictions.classifer.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions for model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.19it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-base/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifer.bias', 'mask_predictions.classifer.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions for model 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.26it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-base/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifer.bias', 'mask_predictions.classifer.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions for model 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.74it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-base/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifer.bias', 'mask_predictions.classifer.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions for model 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.29it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = inference(MODEL_PATHS, test_loader, CONFIG['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51dbed27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T13:37:01.694607Z",
     "iopub.status.busy": "2022-07-09T13:37:01.693954Z",
     "iopub.status.idle": "2022-07-09T13:37:01.702636Z",
     "shell.execute_reply": "2022-07-09T13:37:01.701406Z"
    },
    "papermill": {
     "duration": 0.023033,
     "end_time": "2022-07-09T13:37:01.704660",
     "exception": false,
     "start_time": "2022-07-09T13:37:01.681627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62027276, 0.33876267, 0.04096456],\n",
       "       [0.8803393 , 0.09604445, 0.02361617],\n",
       "       [0.79070073, 0.17820826, 0.03109107],\n",
       "       [0.8160946 , 0.12804925, 0.05585612],\n",
       "       [0.7913507 , 0.15905914, 0.04959013],\n",
       "       [0.58156335, 0.3754637 , 0.04297299],\n",
       "       [0.47678494, 0.49285156, 0.03036351],\n",
       "       [0.77574193, 0.18243572, 0.04182231],\n",
       "       [0.52919805, 0.4350275 , 0.0357745 ],\n",
       "       [0.76650774, 0.19876778, 0.03472452]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75311134",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T13:37:01.723524Z",
     "iopub.status.busy": "2022-07-09T13:37:01.722975Z",
     "iopub.status.idle": "2022-07-09T13:37:01.741137Z",
     "shell.execute_reply": "2022-07-09T13:37:01.740045Z"
    },
    "papermill": {
     "duration": 0.030405,
     "end_time": "2022-07-09T13:37:01.743784",
     "exception": false,
     "start_time": "2022-07-09T13:37:01.713379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>Ineffective</th>\n",
       "      <th>Adequate</th>\n",
       "      <th>Effective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a261b6e14276</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5a88900e7dc1</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9790d835736b</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75ce6d68b67b</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93578d946723</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id  Ineffective  Adequate  Effective\n",
       "0  a261b6e14276         0.20      0.60       0.40\n",
       "1  5a88900e7dc1         3.00      6.00       1.00\n",
       "2  9790d835736b         1.00      2.00       3.00\n",
       "3  75ce6d68b67b         0.33      0.34       0.33\n",
       "4  93578d946723         0.01      0.24       0.47"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv(\"../input/feedback-prize-effectiveness/sample_submission.csv\")\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2dffd5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T13:37:01.762813Z",
     "iopub.status.busy": "2022-07-09T13:37:01.762492Z",
     "iopub.status.idle": "2022-07-09T13:37:01.777869Z",
     "shell.execute_reply": "2022-07-09T13:37:01.776804Z"
    },
    "papermill": {
     "duration": 0.027339,
     "end_time": "2022-07-09T13:37:01.780071",
     "exception": false,
     "start_time": "2022-07-09T13:37:01.752732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>Ineffective</th>\n",
       "      <th>Adequate</th>\n",
       "      <th>Effective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a261b6e14276</td>\n",
       "      <td>0.040965</td>\n",
       "      <td>0.620273</td>\n",
       "      <td>0.338763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5a88900e7dc1</td>\n",
       "      <td>0.023616</td>\n",
       "      <td>0.880339</td>\n",
       "      <td>0.096044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9790d835736b</td>\n",
       "      <td>0.031091</td>\n",
       "      <td>0.790701</td>\n",
       "      <td>0.178208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75ce6d68b67b</td>\n",
       "      <td>0.055856</td>\n",
       "      <td>0.816095</td>\n",
       "      <td>0.128049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93578d946723</td>\n",
       "      <td>0.049590</td>\n",
       "      <td>0.791351</td>\n",
       "      <td>0.159059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id  Ineffective  Adequate  Effective\n",
       "0  a261b6e14276     0.040965  0.620273   0.338763\n",
       "1  5a88900e7dc1     0.023616  0.880339   0.096044\n",
       "2  9790d835736b     0.031091  0.790701   0.178208\n",
       "3  75ce6d68b67b     0.055856  0.816095   0.128049\n",
       "4  93578d946723     0.049590  0.791351   0.159059"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['Adequate'] = preds[:, 0]\n",
    "sample['Effective'] = preds[:, 1]\n",
    "sample['Ineffective'] = preds[:, 2]\n",
    "\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7704c76b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T13:37:01.799044Z",
     "iopub.status.busy": "2022-07-09T13:37:01.798385Z",
     "iopub.status.idle": "2022-07-09T13:37:01.806563Z",
     "shell.execute_reply": "2022-07-09T13:37:01.805515Z"
    },
    "papermill": {
     "duration": 0.02012,
     "end_time": "2022-07-09T13:37:01.808975",
     "exception": false,
     "start_time": "2022-07-09T13:37:01.788855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 87.406208,
   "end_time": "2022-07-09T13:37:05.140063",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-09T13:35:37.733855",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
