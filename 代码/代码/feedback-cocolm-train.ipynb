{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %%\nimport os\nimport gc\nimport copy\nimport time\nimport random\nimport string\nimport joblib\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\n# Utils\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# Sklearn Imports\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GroupKFold, KFold\nfrom sklearn.metrics import f1_score, log_loss\n# For Transformer Models\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, AdamW\nfrom transformers import DataCollatorWithPadding\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\n\nb_ = Fore.BLUE\ny_ = Fore.YELLOW\nsr_ = Style.RESET_ALL\n\n# Suppress warnings\nimport warnings\nfrom torch.utils.checkpoint import checkpoint\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n# %%\nimport wandb\nimport logging\ntry:\n    from kaggle_secrets import UserSecretsClient\n\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    anony = None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your '\n          'W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token '\n          'from here: https://wandb.ai/authorize')\nimport sys\nsys.path.insert(1, '../input/cocolm/huggingface/')\n \nfrom cocolm.modeling_cocolm import COCOLMModel, COCOLMPreTrainedModel\nfrom cocolm.configuration_cocolm import COCOLMConfig\nfrom cocolm.tokenization_cocolm import COCOLMTokenizer\n\n\n# %%\ndef id_generator(size=12, chars=string.ascii_lowercase + string.digits):\n    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))\n\n\nHASH_NAME = id_generator(size=12)\nprint(HASH_NAME)\n# %%\nTRAIN_DIR = \"../input/feedback-prize-effectiveness/train\"\nTEST_DIR = \"../input/feedback-prize-effectiveness/test\"\n# %%\nCONFIG = {\"seed\": 42,\n          \"epochs\": 3,\n          \"model_name\": \"microsoft/cocolm-large\",\n          \"train_batch_size\": 1,\n          \"valid_batch_size\": 2,\n          \"max_length\": 512,\n          \"learning_rate\": 1e-5,\n          \"scheduler\": 'CosineAnnealingLR',\n          \"min_lr\": 1e-6,\n          \"T_max\": 500,\n          \"weight_decay\": 1e-6,\n          \"n_fold\": 4,\n          \"n_accumulate\": 1,\n          \"num_classes\": 3,\n          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n          \"hash_name\": HASH_NAME,\n          \"competition\": \"FeedBack\",\n          \"_wandb_kernel\": \"deb\",\n          \"fc_dropout\":0.2,\n          \"gradient_checkpoint\" : False,\n          }\n# num_workers = 1\n# path = \"../input/feedback-deberta-large-051/\"\n# config_path = path + 'config.pth'\n# model = \"microsoft/deberta-large\"\n# batch_size = 16\n# fc_dropout = 0.2\n# target_size = 3\n# max_len = 512\n# seed = 42\n# n_fold = 4\n# trn_fold = [i for i in range(n_fold)]\n# gradient_checkpoint = False\nCONFIG[\"tokenizer\"] = COCOLMTokenizer.from_pretrained(CONFIG['model_name'])\nCONFIG['group'] = f'{HASH_NAME}-Baseline'\n\n\n# %%\ndef set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n\nset_seed(CONFIG['seed'])\n\n\n# %%\ndef get_essay(essay_id):\n    essay_path = os.path.join(TRAIN_DIR, f\"{essay_id}.txt\")\n    essay_text = open(essay_path, 'r').read()\n    return essay_text\n\n\n# %%\n# ====================================================\n# Utils\n# ====================================================\n\n\n\ndef softmax(z):\n    assert len(z.shape) == 2\n    s = np.max(z, axis=1)\n    s = s[:, np.newaxis] # necessary step to do broadcasting\n    e_x = np.exp(z - s)\n    div = np.sum(e_x, axis=1)\n    div = div[:, np.newaxis] # dito\n    return e_x / div\n\ndef get_score(y_true, y_pred):\n    y_pred = softmax(y_pred)\n    score = log_loss(y_true, y_pred)\n    return round(score, 5)\ndf = pd.read_csv(\"../input/feedback-prize-effectiveness/train.csv\")\ndf['essay_text'] = df['essay_id'].apply(get_essay)\ndf.head()\n\n# %%\n# %%\n\ngkf = GroupKFold(n_splits=CONFIG['n_fold'])\n\nfor fold, (_, val_) in enumerate(gkf.split(X=df, groups=df.essay_id)):\n    df.loc[val_, \"kfold\"] = int(fold)\n\ndf[\"kfold\"] = df[\"kfold\"].astype(int)\ndf.head()\n\n# %%\n\ndf.groupby('kfold')['discourse_effectiveness'].value_counts()\n\n# %%\nfrom text_unidecode import unidecode\nfrom typing import Dict, List, Tuple\nimport codecs\n\ndef replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n\n\ndef replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n\n# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\ncodecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\ncodecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n\ndef resolve_encodings_and_normalize(text: str) -> str:\n    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n    text = (\n        text.encode(\"raw_unicode_escape\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n    )\n    text = unidecode(text)\n    return text\ndf['discourse_text'] = df['discourse_text'].apply(lambda x : resolve_encodings_and_normalize(x))\ndf['essay_text'] = df['essay_text'].apply(lambda x : resolve_encodings_and_normalize(x))\nencoder = LabelEncoder()\ndf['discourse_effectiveness'] = encoder.fit_transform(df['discourse_effectiveness'])\n\nwith open(\"le.pkl\", \"wb\") as fp:\n    joblib.dump(encoder, fp)\n\n\n# %%\n\nclass FeedBackDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.discourse = df['discourse_text'].values\n        self.essay = df['essay_text'].values\n        self.targets = df['discourse_effectiveness'].values\n        self.discourse_type = df['discourse_type'].values\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        discourse = self.discourse[index]\n        discourse_type = self.discourse_type[index]\n        essay = self.essay[index]\n        text = discourse_type+' '+discourse +self.tokenizer.sep_token+essay\n        # _discourse_type = self.tokenizer.encode(discourse_type, add_special_tokens=False)\n        # _discourse = self.tokenizer.encode(discourse, add_special_tokens=False)\n        # _essay = self.tokenizer.encode(essay, add_special_tokens=False)\n        inputs = self.tokenizer.encode_plus(\n            text,\n            # truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            # padding='max_length'\n        )\n\n        return {\n            'input_ids': inputs['input_ids'],\n            # 'attention_mask': inputs['attention_mask'],\n            'target': self.targets[index]\n        }\n\n\n#%%\nclass Collate:\n    def __init__(self, tokenizer, isTrain=True):\n        self.tokenizer = tokenizer\n        self.isTrain = isTrain\n        # self.args = args\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n        # output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n        if self.isTrain:\n            output[\"target\"] = [sample[\"target\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n            # output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n        else:\n            output[\"input_ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"input_ids\"]]\n            # output[\"attention_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"attention_mask\"]]\n\n        # convert to tensors\n        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n        # output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n        if self.isTrain:\n            output[\"target\"] = torch.tensor(output[\"target\"], dtype=torch.long)\n\n        return output\n\ncollate_fn = Collate(tokenizer=CONFIG['tokenizer'], isTrain=True)\n#collate_fn = DataCollatorWithPadding(tokenizer=CONFIG['tokenizer'])\n# %%\n\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\n\nclass MeanMaxPooling(nn.Module):\n    def __init__(self):\n        super(MeanMaxPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        mean_pooling_embeddings = torch.mean(last_hidden_state, 1)\n        _, max_pooling_embeddings = torch.max(last_hidden_state, 1)\n        mean_max_embeddings = torch.cat((mean_pooling_embeddings, max_pooling_embeddings), 1)\n        return mean_max_embeddings\n\n\nclass LSTMPooling(nn.Module):\n    def __init__(self, num_layers, hidden_size, hiddendim_lstm):\n        super(LSTMPooling, self).__init__()\n        self.num_hidden_layers = num_layers\n        self.hidden_size = hidden_size\n        self.hiddendim_lstm = hiddendim_lstm\n        self.lstm = nn.LSTM(self.hidden_size, self.hiddendim_lstm, batch_first=True)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, all_hidden_states):\n        ## forward\n        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n                                     for layer_i in range(1, self.num_hidden_layers + 1)], dim=-1)\n        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n        out, _ = self.lstm(hidden_states, None)\n        out = self.dropout(out[:, -1, :])\n        return out\n\n\nclass WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights=None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n            torch.tensor([1] * (num_hidden_layers + 1 - layer_start), dtype=torch.float)\n        )\n\n    def forward(self, all_hidden_states):\n        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n        return weighted_average\n# %%\nfrom torch.cuda.amp import autocast,GradScaler\nclass FeedBackModel(nn.Module):\n    def __init__(self, model_name):\n        super(FeedBackModel, self).__init__()\n        self.cfg = CONFIG\n        self.config = COCOLMConfig.from_pretrained(model_name)\n        self.model = COCOLMModel.from_pretrained(model_name,config=self.config)\n        # gradient checkpointing  梯度检查点\n        if CONFIG['gradient_checkpoint']:\n            self.model.gradient_checkpointing_enable()\n            print(f\"Gradient Checkpointing: {self.model.is_gradient_checkpointing}\")\n        self.bilstm = nn.LSTM(self.config.hidden_size, (self.config.hidden_size) // 2, num_layers=2,\n                              dropout=self.config.hidden_dropout_prob, batch_first=True,\n                              bidirectional=True)\n\n        self.dropout = nn.Dropout(0.2)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n        #-----------------\n        #self.pooler=WeightedLayerPooling(self.config.num_hidden_layers)\n        self.pooler=MeanPooling()\n\n        self.output = nn.Sequential(\n            nn.Linear(self.config.hidden_size, CONFIG['num_classes'])\n            # nn.Linear(256, self.cfg.target_size)\n        )\n\n    def loss(self, outputs, targets):\n        loss_fct = nn.CrossEntropyLoss()\n        loss = loss_fct(outputs, targets)\n        return loss\n\n    def monitor_metrics(self, outputs, targets):\n        device = targets.get_device()\n        # print(outputs)\n        # print(targets)\n        mll = log_loss(\n            targets.cpu().detach().numpy(),\n            softmax(outputs.cpu().detach().numpy()),\n            labels=[0, 1, 2],\n        )\n        return mll\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    # def forward(self, ids, mask):\n    #     out = self.model(input_ids=ids, attention_mask=mask,\n    #                      output_hidden_states=False)\n    #     out = self.pooler(out.last_hidden_state, mask)\n    #     out = self.drop(out)\n    #     outputs = self.fc(out)\n    #     return outputs\n    def forward(self, ids, token_type_ids=None, targets=None):\n        if token_type_ids:\n            transformer_out = self.model(ids,  token_type_ids)\n        else:\n            transformer_out = self.model(ids)\n\n        # LSTM/GRU header\n        # all_hidden_states = torch.stack(transformer_out[1])\n        # sequence_output = self.pooler(all_hidden_states)3\n\n        # simple CLS\n\n        sequence_output = transformer_out[0][:, 0, :]\n        #all_hidden_states = torch.stack(transformer_out[1])\n        #sequence_output=self.pooler(all_hidden_states)\n#         all_hidden_states = torch.stack(transformer_out[1])\n\n#         concatenate_pooling = torch.cat(\n#             (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]), -1\n#         )\n#         sequence_output = concatenate_pooling[:, 0]\n        # Main task\n        logits1 = self.output(self.dropout1(sequence_output))\n        logits2 = self.output(self.dropout2(sequence_output))\n        logits3 = self.output(self.dropout3(sequence_output))\n        logits4 = self.output(self.dropout4(sequence_output))\n        logits5 = self.output(self.dropout5(sequence_output))\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n\n        if targets is not None:\n            metric = self.monitor_metrics(logits, targets)\n            return logits\n\n        return logits\n\n\n# %%\n\ndef criterion(outputs, labels):\n    return nn.CrossEntropyLoss()(outputs, labels)\n\n\n# %%\n\ndef train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n\n    dataset_size = 0\n    running_loss = 0.0\n    scaler = GradScaler()\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        ids = data['input_ids'].to(device, dtype=torch.long)\n        # mask = data['attention_mask'].to(device, dtype=torch.long)\n        targets = data['target'].to(device, dtype=torch.long)\n\n        batch_size = ids.size(0)\n\n#         outputs = model(ids)\n\n        with autocast(enabled=True):\n            outputs = model(ids)\n            \n        loss = criterion(outputs, targets)\n        loss = loss / CONFIG['n_accumulate']\n        scaler.scale(loss).backward()\n#         loss.backward()\n\n        if (step + 1) % CONFIG['n_accumulate'] == 0:\n#             optimizer.step()\n            scaler.unscale_(optimizer)\n            scaler.step(optimizer)\n            scaler.update()\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            if scheduler is not None:\n                scheduler.step()\n\n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n\n        epoch_loss = running_loss / dataset_size\n\n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n\n    return epoch_loss\n\n\n# %%\n\n@torch.no_grad()\ndef valid_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n\n    dataset_size = 0\n    running_loss = 0.0\n\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        ids = data['input_ids'].to(device, dtype=torch.long)\n        # mask = data['attention_mask'].to(device, dtype=torch.long)\n        targets = data['target'].to(device, dtype=torch.long)\n\n        batch_size = ids.size(0)\n\n        outputs = model(ids)\n\n        loss = criterion(outputs, targets)\n\n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n\n        epoch_loss = running_loss / dataset_size\n\n        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n\n    gc.collect()\n\n    return epoch_loss\n\n\n# %%\n\ndef run_training(model, optimizer, scheduler, device, num_epochs, fold):\n    # To automatically log gradients\n    # wandb.watch(model, log_freq=100)\n\n    if torch.cuda.is_available():\n        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n\n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_epoch_loss = np.inf\n    history = defaultdict(list)\n\n    for epoch in range(1, num_epochs + 1):\n        gc.collect()\n        train_epoch_loss = train_one_epoch(model, optimizer, scheduler,\n                                           dataloader=train_loader,\n                                           device=CONFIG['device'], epoch=epoch)\n\n        val_epoch_loss = valid_one_epoch(model, valid_loader, device=CONFIG['device'],\n                                         epoch=epoch)\n\n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(val_epoch_loss)\n\n        # Log the metrics\n        logging.warning({\"Train Loss\": train_epoch_loss})\n        logging.warning({\"Valid Loss\": val_epoch_loss})\n        # wandb.log({\"Train Loss\": train_epoch_loss})\n        # wandb.log({\"Valid Loss\": val_epoch_loss})\n\n        # deep copy the model\n        if val_epoch_loss <= best_epoch_loss:\n            print(f\"{b_}Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n            best_epoch_loss = val_epoch_loss\n            # run.summary[\"Best Loss\"] = best_epoch_loss\n            logging.warning({\"Best Loss\": best_epoch_loss})\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = f\"Loss-Fold-{fold}.bin\"\n            torch.save(model.state_dict(), PATH)\n            # Save a model file from the current directory\n            print(f\"Model Saved{sr_}\")\n\n        print()\n\n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n\n    return model, history\n\n\n# %%\n\ndef prepare_loaders(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n\n    train_dataset = FeedBackDataset(df_train, tokenizer=CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n    valid_dataset = FeedBackDataset(df_valid, tokenizer=CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], collate_fn=collate_fn,\n                              num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], collate_fn=collate_fn,\n                              num_workers=2, shuffle=False, pin_memory=True)\n\n    return train_loader, valid_loader\n\n\n# %%\n\ndef fetch_scheduler(optimizer):\n    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['T_max'],\n                                                   eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CONFIG['T_0'],\n                                                             eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == None:\n        return None\n\n    return scheduler\n\n\n# %%\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-17T18:15:29.780235Z","iopub.execute_input":"2022-07-17T18:15:29.780605Z","iopub.status.idle":"2022-07-17T18:16:11.574348Z","shell.execute_reply.started":"2022-07-17T18:15:29.780574Z","shell.execute_reply":"2022-07-17T18:16:11.573357Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    torch.cuda.empty_cache()\n    for fold in range(0, CONFIG['n_fold']):\n        print(f\"{y_}====== Fold: {fold} ======{sr_}\")\n        #  run = wandb.init(project='FeedBack',\n        #                  config=CONFIG,\n        #                  job_type='Train',\n        #                  group=CONFIG['group'],\n        #                  tags=[CONFIG['model_name'], f'{HASH_NAME}'],\n        #                  name=f'{HASH_NAME}-fold-{fold}',\n        #                  anonymous='must')\n\n        # Create Dataloaders\n        train_loader, valid_loader = prepare_loaders(fold=fold)\n\n        model = FeedBackModel(CONFIG['model_name'])\n        model.to(CONFIG['device'])\n\n        # Define Optimizer and Scheduler\n        optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n        scheduler = fetch_scheduler(optimizer)\n\n        model, history = run_training(model, optimizer, scheduler,\n                                      device=CONFIG['device'],\n                                      num_epochs=CONFIG['epochs'],\n                                      fold=fold)\n\n        # run.finish()\n\n        del model, history, train_loader, valid_loader\n        _ = gc.collect()\n        print()","metadata":{"execution":{"iopub.status.busy":"2022-07-17T18:16:11.576217Z","iopub.execute_input":"2022-07-17T18:16:11.578026Z","iopub.status.idle":"2022-07-17T18:16:17.148115Z","shell.execute_reply.started":"2022-07-17T18:16:11.577988Z","shell.execute_reply":"2022-07-17T18:16:17.146712Z"},"trusted":true},"execution_count":12,"outputs":[]}]}