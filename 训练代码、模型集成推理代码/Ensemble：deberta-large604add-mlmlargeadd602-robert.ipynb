{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Ensemble of models\n\n1. deberta-large-text-scls-fgm-add2 Score: 0.604\n\n2. deberta-v3-large-text-4cls-mlm-fgm01 Score: 0.602\n\n3. robert-base / Score:0.649\n\n\n#### If this notebook is helpful, please upvote the original versions:\n\n\n## Content\n\n> I tried to get the same result for each model, but by unifying the actions and removing the excess.\n>  \n> For these models, I use the same prepare_input and inference_fn.\n\n```\ndef prepare_input(cfg, text, text_2=None):\n    inputs = cfg.tokenizer(text, text_2,\n                           padding=\"max_length\",\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           truncation=True)\n\n    [...]\n\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    \n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n            \n        with torch.no_grad():\n            output = model(inputs)\n        \n    [...]\n```\n","metadata":{"papermill":{"duration":0.017223,"end_time":"2022-06-29T02:53:47.264285","exception":false,"start_time":"2022-06-29T02:53:47.247062","status":"completed"},"tags":[]}},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"markdown","source":"# 1. Import & Def & Set & Load","metadata":{"papermill":{"duration":0.013915,"end_time":"2022-06-29T02:53:47.292714","exception":false,"start_time":"2022-06-29T02:53:47.278799","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import gc\nimport os\nimport pickle\nimport glob\n\nfrom text_unidecode import unidecode\nfrom typing import Dict, List, Tuple\nimport codecs\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\n\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.nn import Parameter\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport re\nfrom datasets import Dataset\nimport copy\nimport time\nimport random\nimport joblib\n\nfrom torch.utils.data import  DataLoader\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":8.806745,"end_time":"2022-06-29T02:53:56.118166","exception":false,"start_time":"2022-06-29T02:53:47.311421","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T05:59:55.496562Z","iopub.execute_input":"2022-08-02T05:59:55.496905Z","iopub.status.idle":"2022-08-02T06:00:03.683569Z","shell.execute_reply.started":"2022-08-02T05:59:55.496825Z","shell.execute_reply":"2022-08-02T06:00:03.682739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_essay(essay_id: str, txt_dir: str):\n    essay_path = os.path.join(COMP_DIR + txt_dir, essay_id + '.txt')\n    essay_text = open(essay_path, 'r').read()\n    \n    return essay_text\n\n\ndef prepare_input(cfg, text, text_2=None):\n    inputs = cfg.tokenizer(text, text_2,\n                           padding=\"max_length\",\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           truncation=True)\n\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    \n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n            \n        with torch.no_grad():\n            output = model(inputs)\n        \n        preds.append(F.softmax(output).to('cpu').numpy())\n\n    return np.concatenate(preds)  \n\n\ndef show_gradient(df, n_row=None):\n    if not n_row:\n        n_row = 5\n\n    return df.head(n_row) \\\n                .assign(all_mean=lambda x: x.mean(axis=1)) \\\n                    .style.background_gradient(cmap=cm, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:00:03.685288Z","iopub.execute_input":"2022-08-02T06:00:03.685933Z","iopub.status.idle":"2022-08-02T06:00:03.697620Z","shell.execute_reply.started":"2022-08-02T06:00:03.685903Z","shell.execute_reply":"2022-08-02T06:00:03.696775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_essay(essay_id):\n    essay_path = os.path.join(TEST_DIR, f\"{essay_id}.txt\")\n    essay_text = open(essay_path, 'r').read()\n    return essay_text\n\ndef softmax(z):\n    assert len(z.shape) == 2\n    s = np.max(z, axis=1)\n    s = s[:, np.newaxis] # necessary step to do broadcasting\n    e_x = np.exp(z - s)\n    div = np.sum(e_x, axis=1)\n    div = div[:, np.newaxis] # dito\n    return e_x / div\n\ndef get_score(y_true, y_pred):\n    y_pred = softmax(y_pred)\n    score = log_loss(y_true, y_pred)\n    return round(score, 5)","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:00:03.698997Z","iopub.execute_input":"2022-08-02T06:00:03.699412Z","iopub.status.idle":"2022-08-02T06:00:03.708584Z","shell.execute_reply.started":"2022-08-02T06:00:03.699377Z","shell.execute_reply":"2022-08-02T06:00:03.707769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.precision', 4)\ncm = sns.light_palette('green', as_cmap=True)\nprops_param = \"color:white; font-weight:bold; background-color:green;\"\n\nN_ROW = 10\n\nCOMP_DIR = \"../input/feedback-prize-effectiveness/\"\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:00:03.712206Z","iopub.execute_input":"2022-08-02T06:00:03.713023Z","iopub.status.idle":"2022-08-02T06:00:03.781686Z","shell.execute_reply.started":"2022-08-02T06:00:03.712987Z","shell.execute_reply":"2022-08-02T06:00:03.780759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_DIR = \"../input/feedback-prize-effectiveness/train\"\nTEST_DIR = \"../input/feedback-prize-effectiveness/test\"","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:00:03.782618Z","iopub.execute_input":"2022-08-02T06:00:03.783332Z","iopub.status.idle":"2022-08-02T06:00:03.792195Z","shell.execute_reply.started":"2022-08-02T06:00:03.783304Z","shell.execute_reply":"2022-08-02T06:00:03.791335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = COMP_DIR + \"test.csv\"\ntrain_path=COMP_DIR + \"train.csv\"\nsubmission_path = COMP_DIR + \"sample_submission.csv\"\n\ntest_origin = pd.read_csv(test_path)\ntrain_origin = pd.read_csv(train_path)\nsubmission_origin = pd.read_csv(submission_path)","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:00:03.795181Z","iopub.execute_input":"2022-08-02T06:00:03.795637Z","iopub.status.idle":"2022-08-02T06:00:04.084312Z","shell.execute_reply.started":"2022-08-02T06:00:03.795611Z","shell.execute_reply":"2022-08-02T06:00:04.083480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_origin = pd.read_csv(\"../input/feedback-prize-effectiveness/test.csv\")\ndf=test_origin.copy()\ndf['essay_text'] = df['essay_id'].apply(get_essay)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:00:04.085521Z","iopub.execute_input":"2022-08-02T06:00:04.085916Z","iopub.status.idle":"2022-08-02T06:00:04.117450Z","shell.execute_reply.started":"2022-08-02T06:00:04.085878Z","shell.execute_reply":"2022-08-02T06:00:04.116690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from text_unidecode import unidecode\nfrom typing import Dict, List, Tuple\nimport codecs\n\ndef replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n\n\ndef replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n\n# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\ncodecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\ncodecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n\ndef resolve_encodings_and_normalize(text: str) -> str:\n    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n    text = (\n        text.encode(\"raw_unicode_escape\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n    )\n    text = unidecode(text)\n    return text\ndf['discourse_text'] = df['discourse_text'].apply(lambda x : resolve_encodings_and_normalize(x))\ndf['essay_text'] = df['essay_text'].apply(lambda x : resolve_encodings_and_normalize(x))\n#encoder = LabelEncoder()\n#df['discourse_effectiveness'] = encoder.fit_transform(df['discourse_effectiveness'])\n\n# with open(\"le.pkl\", \"wb\") as fp:\n#     joblib.dump(encoder, fp)","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:00:04.118981Z","iopub.execute_input":"2022-08-02T06:00:04.119305Z","iopub.status.idle":"2022-08-02T06:00:04.138577Z","shell.execute_reply.started":"2022-08-02T06:00:04.119271Z","shell.execute_reply":"2022-08-02T06:00:04.137775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.02732,"end_time":"2022-06-29T02:53:56.154308","exception":false,"start_time":"2022-06-29T02:53:56.126988","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['essay_text']\ndisc_types = [\n    \"Claim\",\n    \"Concluding Statement\",\n    \"Counterclaim\",\n    \"Evidence\",\n    \"Lead\",\n    \"Position\",\n    \"Rebuttal\",\n]\ncls_tokens_map = {label: f\"[{label}]\" for label in disc_types}\nend_tokens_map = {label: f\"[{label}]\" for label in disc_types}\ndef find_positions(example):\n    text = example[\"text\"][0]\n\n    # keeps track of what has already\n    # been located\n    min_idx = 0\n\n    # stores start and end indexes of discourse_texts\n    idxs = []\n\n    for dt in example[\"discourse_text\"]:\n        # calling strip is essential\n        matches = list(re.finditer(re.escape(dt.strip()), text))\n\n        # If there are multiple matches, take the first one\n        # that is past the previous discourse texts.\n        if len(matches) > 1:\n            for m in matches:\n                if m.start() >= min_idx:\n                    break\n        # If no matches are found\n        elif len(matches) == 0:\n            idxs.append([-1])  # will filter out later\n            continue\n            # If one match is found\n        else:\n            m = matches[0]\n\n        idxs.append([m.start(), m.end()])\n\n        min_idx = m.start()\n\n    return idxs\n\n\ndef tokenize(example):\n    example[\"idxs\"] = find_positions(example)\n\n    text = example[\"text\"][0]\n    chunks = []\n    labels = []\n    prev = 0\n\n    zipped = zip(\n        example[\"idxs\"],\n        example[\"discourse_type\"],\n#         example[\"discourse_effectiveness\"],\n    )\n    for idxs, disc_type in zipped:\n        # when the discourse_text wasn't found\n        if idxs == [-1]:\n            continue\n\n        s, e = idxs\n\n        # if the start of the current discourse_text is not\n        # at the end of the previous one.\n        # (text in between discourse_texts)\n        if s != prev:\n            chunks.append(text[prev:s])\n            prev = s\n\n        # if the start of the current discourse_text is\n        # the same as the end of the previous discourse_text\n        if s == prev:\n            chunks.append(cls_tokens_map[disc_type])\n            chunks.append(text[s:e])\n            chunks.append(end_tokens_map[disc_type])\n\n        prev = e\n\n\n\n\n    # at this point, labels is not the same shape as input_ids.\n    # The following loop will add -100 so that the loss function\n    # ignores all tokens except CLS tokens\n\n\n\n\n\n\n\n\n\n    a=\" \".join(chunks)\n    example[\"text\"][0]=a\n\n\n\n\n\n\n\n\n    return example\n# %%\ngrouped = df.groupby([\"essay_id\"]).agg(list)\n\nnum=range(0,grouped.shape[0])\ngrouped[\"num\"]=num\nds = Dataset.from_pandas(grouped)\nds = ds.map(\n    tokenize,\n    batched=False,\n\n)\nx=ds[\"text\"]\n\nfor i in range(df.shape[0]):\n    a1=grouped.loc[df[\"essay_id\"][i]][\"num\"]\n    b1=x[a1][0]\n    df[\"text\"][i]=b1\n\ndel ds,grouped,x","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:00:04.139941Z","iopub.execute_input":"2022-08-02T06:00:04.140584Z","iopub.status.idle":"2022-08-02T06:00:04.243447Z","shell.execute_reply.started":"2022-08-02T06:00:04.140550Z","shell.execute_reply":"2022-08-02T06:00:04.242751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.08735,"end_time":"2022-06-29T02:53:56.250659","exception":false,"start_time":"2022-06-29T02:53:56.163309","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"text\"][1]","metadata":{"papermill":{"duration":0.030936,"end_time":"2022-06-29T02:53:56.290445","exception":false,"start_time":"2022-06-29T02:53:56.259509","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:00:04.246421Z","iopub.execute_input":"2022-08-02T06:00:04.246690Z","iopub.status.idle":"2022-08-02T06:00:04.252040Z","shell.execute_reply.started":"2022-08-02T06:00:04.246665Z","shell.execute_reply":"2022-08-02T06:00:04.251203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.027815,"end_time":"2022-06-29T02:53:56.326953","exception":false,"start_time":"2022-06-29T02:53:56.299138","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Extract predictions","metadata":{"papermill":{"duration":0.00859,"end_time":"2022-06-29T02:53:56.344023","exception":false,"start_time":"2022-06-29T02:53:56.335433","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 3.1 DeBerta","metadata":{"papermill":{"duration":0.008456,"end_time":"2022-06-29T02:53:56.361487","exception":false,"start_time":"2022-06-29T02:53:56.353031","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"MODEL_DIR='../input/large-text-scls-fgmadd2'\nMODEL_PATHS = [\n    f'{MODEL_DIR}/Loss-Fold-0.bin',\n    f'{MODEL_DIR}/Loss-Fold-1.bin',\n    f'{MODEL_DIR}/Loss-Fold-2.bin',\n    f'{MODEL_DIR}/Loss-Fold-3.bin',\n    f'{MODEL_DIR}/Loss-Fold-4.bin',\n]\nTRAIN_DIR = \"../input/feedback-prize-effectiveness/train\"\nTEST_DIR = \"../input/feedback-prize-effectiveness/test\"\nCONFIG = dict(\n    seed = 42,\n    model_name = '../input/deberta-v3-large/deberta-v3-large',\n    test_batch_size = 16,\n    max_length = 512,\n    num_classes = 3,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n    gradient_checkpoint=False\n)\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.029514,"end_time":"2022-06-29T02:53:56.401394","exception":false,"start_time":"2022-06-29T02:53:56.37188","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:00:04.253251Z","iopub.execute_input":"2022-08-02T06:00:04.254165Z","iopub.status.idle":"2022-08-02T06:00:04.940644Z","shell.execute_reply.started":"2022-08-02T06:00:04.254130Z","shell.execute_reply":"2022-08-02T06:00:04.939750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(f\"../input/{MODEL_DIR}/le.pkl\", \"rb\") as fp:\n    encoder = joblib.load(fp)\n    \nencoder.classes_","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:00:04.942147Z","iopub.execute_input":"2022-08-02T06:00:04.942500Z","iopub.status.idle":"2022-08-02T06:00:04.989532Z","shell.execute_reply.started":"2022-08-02T06:00:04.942463Z","shell.execute_reply":"2022-08-02T06:00:04.988553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedBackDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.discourse_type = df['discourse_type'].values\n        self.discourse = df['discourse_text'].values\n        self.essay = df['essay_text'].values\n        self.essay_type = df['text'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        discourse_type = self.discourse_type[index]\n        discourse = self.discourse[index]\n        essay_type = self.essay_type[index]\n        essay = self.essay[index]\n        text = discourse_type+' '+discourse +self.tokenizer.sep_token+essay_type\n        inputs = self.tokenizer.encode_plus(\n                        text,\n                        truncation=True,\n                        add_special_tokens=True,\n                        max_length=self.max_len,\n                        padding='max_length'\n                    )\n        \n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        \n        return {\n            'input_ids': inputs['input_ids'],\n            'attention_mask': inputs['attention_mask']\n        }","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:00:04.990890Z","iopub.execute_input":"2022-08-02T06:00:04.991263Z","iopub.status.idle":"2022-08-02T06:00:05.000068Z","shell.execute_reply.started":"2022-08-02T06:00:04.991227Z","shell.execute_reply":"2022-08-02T06:00:04.999258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Collate:\n    def __init__(self, tokenizer, isTrain=True):\n        self.tokenizer = tokenizer\n        self.isTrain = isTrain\n        # self.args = args\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n        if self.isTrain:\n            output[\"target\"] = [sample[\"target\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n            output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n        else:\n            output[\"input_ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"input_ids\"]]\n            output[\"attention_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"attention_mask\"]]\n\n        # convert to tensors\n        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n        if self.isTrain:\n            output[\"target\"] = torch.tensor(output[\"target\"], dtype=torch.long)\n\n        return output\n\ncollate_fn = Collate(CONFIG[\"tokenizer\"], isTrain=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:00:05.001417Z","iopub.execute_input":"2022-08-02T06:00:05.002064Z","iopub.status.idle":"2022-08-02T06:00:05.015080Z","shell.execute_reply.started":"2022-08-02T06:00:05.002029Z","shell.execute_reply":"2022-08-02T06:00:05.014042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\n\ntest_dataset = FeedBackDataset(df, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                         num_workers=2, shuffle=False,collate_fn=collate_fn, pin_memory=True)","metadata":{"papermill":{"duration":0.054662,"end_time":"2022-06-29T02:53:56.657806","exception":false,"start_time":"2022-06-29T02:53:56.603144","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:00:05.016559Z","iopub.execute_input":"2022-08-02T06:00:05.017082Z","iopub.status.idle":"2022-08-02T06:00:05.028064Z","shell.execute_reply.started":"2022-08-02T06:00:05.016947Z","shell.execute_reply":"2022-08-02T06:00:05.027340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\n\nclass MeanMaxPooling(nn.Module):\n    def __init__(self):\n        super(MeanMaxPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        mean_pooling_embeddings = torch.mean(last_hidden_state, 1)\n        _, max_pooling_embeddings = torch.max(last_hidden_state, 1)\n        mean_max_embeddings = torch.cat((mean_pooling_embeddings, max_pooling_embeddings), 1)\n        return mean_max_embeddings\n\n\nclass LSTMPooling(nn.Module):\n    def __init__(self, num_layers, hidden_size, hiddendim_lstm):\n        super(LSTMPooling, self).__init__()\n        self.num_hidden_layers = num_layers\n        self.hidden_size = hidden_size\n        self.hiddendim_lstm = hiddendim_lstm\n        self.lstm = nn.LSTM(self.hidden_size, self.hiddendim_lstm, batch_first=True)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, all_hidden_states):\n        ## forward\n        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n                                     for layer_i in range(1, self.num_hidden_layers + 1)], dim=-1)\n        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n        out, _ = self.lstm(hidden_states, None)\n        out = self.dropout(out[:, -1, :])\n        return out\n\n\nclass WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights=None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n            torch.tensor([1] * (num_hidden_layers + 1 - layer_start), dtype=torch.float)\n        )\n\n    def forward(self, all_hidden_states):\n        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n        return weighted_average","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:00:05.029578Z","iopub.execute_input":"2022-08-02T06:00:05.030002Z","iopub.status.idle":"2022-08-02T06:00:05.047163Z","shell.execute_reply.started":"2022-08-02T06:00:05.029910Z","shell.execute_reply":"2022-08-02T06:00:05.046361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedBackModel(nn.Module):\n    def __init__(self, model_name):\n        super(FeedBackModel, self).__init__()\n        self.cfg = CONFIG\n        self.config = AutoConfig.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name,config=self.config)\n        # gradient checkpointing  梯度检查点\n        if CONFIG['gradient_checkpoint']:\n            self.model.gradient_checkpointing_enable()\n            print(f\"Gradient Checkpointing: {self.model.is_gradient_checkpointing}\")\n        self.bilstm = nn.LSTM(self.config.hidden_size, (self.config.hidden_size) // 2, num_layers=2,\n                              dropout=self.config.hidden_dropout_prob, batch_first=True,\n                              bidirectional=True)\n\n        self.dropout = nn.Dropout(0.2)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n        #-----------------\n        #self.pooler=WeightedLayerPooling(self.config.num_hidden_layers)\n        #self.pooler=MeanPooling()\n\n        self.output = nn.Sequential(\n            nn.Linear(self.config.hidden_size, CONFIG['num_classes'])\n            # nn.Linear(256, self.cfg.target_size)\n        )\n\n    def loss(self, outputs, targets):\n        loss_fct = nn.CrossEntropyLoss()\n        loss = loss_fct(outputs, targets)\n        return loss\n\n    def monitor_metrics(self, outputs, targets):\n        device = targets.get_device()\n        # print(outputs)\n        # print(targets)\n        mll = log_loss(\n            targets.cpu().detach().numpy(),\n            softmax(outputs.cpu().detach().numpy()),\n            labels=[0, 1, 2],\n        )\n        return mll\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    # def forward(self, ids, mask):\n    #     out = self.model(input_ids=ids, attention_mask=mask,\n    #                      output_hidden_states=False)\n    #     out = self.pooler(out.last_hidden_state, mask)\n    #     out = self.drop(out)\n    #     outputs = self.fc(out)\n    #     return outputs\n    def forward(self, ids, mask, token_type_ids=None, targets=None):\n        if token_type_ids:\n            transformer_out = self.model(ids, mask, token_type_ids)\n        else:\n            transformer_out = self.model(ids, mask)\n\n        # LSTM/GRU header\n        # all_hidden_states = torch.stack(transformer_out[1])\n        # sequence_output = self.pooler(all_hidden_states)3\n#         all_hidden_states = torch.stack(transformer_out[1])\n\n#         concatenate_pooling = torch.cat(\n#             (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]), -1\n#         )\n#         sequence_output = concatenate_pooling[:, 0]\n        # simple CLS\n        #transformer_out = self.pooler(transformer_out.last_hidden_state,mask)\n        #sequence_output = transformer_out\n        sequence_output = transformer_out[0][:, 0, :]\n        #all_hidden_states = torch.stack(transformer_out[1])\n        #sequence_output=self.pooler(all_hidden_states)\n\n        # Main task\n        logits1 = self.output(self.dropout1(sequence_output))\n        logits2 = self.output(self.dropout2(sequence_output))\n        logits3 = self.output(self.dropout3(sequence_output))\n        logits4 = self.output(self.dropout4(sequence_output))\n        logits5 = self.output(self.dropout5(sequence_output))\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n\n        if targets is not None:\n            metric = self.monitor_metrics(logits, targets)\n            return logits\n\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:00:05.048503Z","iopub.execute_input":"2022-08-02T06:00:05.048941Z","iopub.status.idle":"2022-08-02T06:00:05.069081Z","shell.execute_reply.started":"2022-08-02T06:00:05.048905Z","shell.execute_reply":"2022-08-02T06:00:05.068215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_fn(model, dataloader, device):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    PREDS = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        ids = data['input_ids'].to(device, dtype = torch.long)\n        mask = data['attention_mask'].to(device, dtype = torch.long)\n        \n        outputs = model(ids, mask)\n        outputs = F.softmax(outputs, dim=1)\n        PREDS.append(outputs.cpu().detach().numpy()) \n    \n    PREDS = np.concatenate(PREDS)\n    gc.collect()\n    \n    return PREDS","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:00:05.070380Z","iopub.execute_input":"2022-08-02T06:00:05.070779Z","iopub.status.idle":"2022-08-02T06:00:05.082499Z","shell.execute_reply.started":"2022-08-02T06:00:05.070741Z","shell.execute_reply":"2022-08-02T06:00:05.081730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference(model_paths, dataloader, device):\n    final_preds = []\n    for i, path in enumerate(model_paths):\n        model = FeedBackModel(CONFIG['model_name'])\n        model.to(CONFIG['device'])\n        model.load_state_dict(torch.load(path))\n        \n        print(f\"Getting predictions for model {i+1}\")\n        preds = valid_fn(model, dataloader, device)\n        final_preds.append(preds)\n        del model, preds\n        torch.cuda.empty_cache()    \n        gc.collect()\n    \n#     final_preds = np.array(final_preds)\n#     final_preds = np.mean(final_preds, axis=0)\n\n\n    return final_preds","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:00:05.083685Z","iopub.execute_input":"2022-08-02T06:00:05.084524Z","iopub.status.idle":"2022-08-02T06:00:05.094200Z","shell.execute_reply.started":"2022-08-02T06:00:05.084494Z","shell.execute_reply":"2022-08-02T06:00:05.093390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deberta_predictions = inference(MODEL_PATHS, test_loader, CONFIG['device'])","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:00:05.095541Z","iopub.execute_input":"2022-08-02T06:00:05.095923Z","iopub.status.idle":"2022-08-02T06:02:25.316527Z","shell.execute_reply.started":"2022-08-02T06:00:05.095874Z","shell.execute_reply":"2022-08-02T06:02:25.315496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndeb_ineffective = []\ndeb_effective = []\ndeb_adequate = []\n\nfor x in deberta_predictions:\n    deb_ineffective.append(x[:, 2])\n    deb_adequate.append(x[:, 0])\n    deb_effective.append(x[:, 1])","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:02:25.318546Z","iopub.execute_input":"2022-08-02T06:02:25.319477Z","iopub.status.idle":"2022-08-02T06:02:25.325045Z","shell.execute_reply.started":"2022-08-02T06:02:25.319433Z","shell.execute_reply":"2022-08-02T06:02:25.324101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# deberta_predictions = np.array(deberta_predictions)\n# deberta_predictions = np.mean(deberta_predictions, axis=0)\n# deberta_predictions.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:02:25.326298Z","iopub.execute_input":"2022-08-02T06:02:25.326925Z","iopub.status.idle":"2022-08-02T06:02:25.338589Z","shell.execute_reply.started":"2022-08-02T06:02:25.326879Z","shell.execute_reply":"2022-08-02T06:02:25.337720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deberta_predictions\n","metadata":{"papermill":{"duration":0.018775,"end_time":"2022-06-29T02:55:43.178719","exception":false,"start_time":"2022-06-29T02:55:43.159944","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:02:25.339864Z","iopub.execute_input":"2022-08-02T06:02:25.340354Z","iopub.status.idle":"2022-08-02T06:02:25.353286Z","shell.execute_reply.started":"2022-08-02T06:02:25.340320Z","shell.execute_reply":"2022-08-02T06:02:25.352410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deb_ineffective = pd.DataFrame(deb_ineffective).T\n\nshow_gradient(\n    deb_ineffective,\n    N_ROW)","metadata":{"papermill":{"duration":0.148585,"end_time":"2022-06-29T02:55:43.337415","exception":false,"start_time":"2022-06-29T02:55:43.18883","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:02:25.354729Z","iopub.execute_input":"2022-08-02T06:02:25.355167Z","iopub.status.idle":"2022-08-02T06:02:25.526484Z","shell.execute_reply.started":"2022-08-02T06:02:25.355132Z","shell.execute_reply":"2022-08-02T06:02:25.525692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deb_adequate = pd.DataFrame(deb_adequate).T\n\nshow_gradient(\n    deb_adequate,\n    N_ROW)","metadata":{"papermill":{"duration":0.040998,"end_time":"2022-06-29T02:55:43.389216","exception":false,"start_time":"2022-06-29T02:55:43.348218","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:02:25.527677Z","iopub.execute_input":"2022-08-02T06:02:25.528208Z","iopub.status.idle":"2022-08-02T06:02:25.555272Z","shell.execute_reply.started":"2022-08-02T06:02:25.528173Z","shell.execute_reply":"2022-08-02T06:02:25.554469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deb_effective = pd.DataFrame(deb_effective).T\n\nshow_gradient(\n    deb_effective,\n    N_ROW)","metadata":{"papermill":{"duration":0.043433,"end_time":"2022-06-29T02:55:43.443852","exception":false,"start_time":"2022-06-29T02:55:43.400419","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:02:25.556481Z","iopub.execute_input":"2022-08-02T06:02:25.556957Z","iopub.status.idle":"2022-08-02T06:02:25.582432Z","shell.execute_reply.started":"2022-08-02T06:02:25.556922Z","shell.execute_reply":"2022-08-02T06:02:25.581697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 base","metadata":{"papermill":{"duration":0.011395,"end_time":"2022-06-29T02:55:43.467154","exception":false,"start_time":"2022-06-29T02:55:43.455759","status":"completed"},"tags":[]}},{"cell_type":"code","source":"MODEL_DIR='../input/large-text-4cls-mlm-fgm01'\nMODEL_PATHS = [\n    f'{MODEL_DIR}/Loss-Fold-0.bin',\n    f'{MODEL_DIR}/Loss-Fold-1.bin',\n    f'{MODEL_DIR}/Loss-Fold-2.bin',\n    f'{MODEL_DIR}/Loss-Fold-3.bin',\n#     f'{MODEL_DIR}/Loss-Fold-4.bin',\n]\nTRAIN_DIR = \"../input/feedback-prize-effectiveness/train\"\nTEST_DIR = \"../input/feedback-prize-effectiveness/test\"\nCONFIG = dict(\n    seed = 42,\n    model_name = '../input/fpdebertalarge',\n    test_batch_size = 16,\n    max_length = 512,\n    num_classes = 3,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n    gradient_checkpoint=False\n)\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.023062,"end_time":"2022-06-29T02:55:43.501615","exception":false,"start_time":"2022-06-29T02:55:43.478553","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:02:25.583709Z","iopub.execute_input":"2022-08-02T06:02:25.584042Z","iopub.status.idle":"2022-08-02T06:02:25.731179Z","shell.execute_reply.started":"2022-08-02T06:02:25.584010Z","shell.execute_reply":"2022-08-02T06:02:25.730330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedBackDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.discourse_type = df['discourse_type'].values\n        self.discourse = df['discourse_text'].values\n        self.essay = df['essay_text'].values\n        self.essay_type = df['text'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        discourse_type = self.discourse_type[index]\n        discourse = self.discourse[index]\n        essay_type = self.essay_type[index]\n        essay = self.essay[index]\n        text = discourse_type+' '+discourse +self.tokenizer.sep_token+essay_type\n        inputs = self.tokenizer.encode_plus(\n                        text,\n                        truncation=True,\n                        add_special_tokens=True,\n                        max_length=self.max_len,\n                        padding='max_length'\n                    )\n        \n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        \n        return {\n            'input_ids': inputs['input_ids'],\n            'attention_mask': inputs['attention_mask']\n        }","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:02:25.736199Z","iopub.execute_input":"2022-08-02T06:02:25.736534Z","iopub.status.idle":"2022-08-02T06:02:25.745065Z","shell.execute_reply.started":"2022-08-02T06:02:25.736505Z","shell.execute_reply":"2022-08-02T06:02:25.744227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Collate:\n    def __init__(self, tokenizer, isTrain=True):\n        self.tokenizer = tokenizer\n        self.isTrain = isTrain\n        # self.args = args\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n        if self.isTrain:\n            output[\"target\"] = [sample[\"target\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n            output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n        else:\n            output[\"input_ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"input_ids\"]]\n            output[\"attention_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"attention_mask\"]]\n\n        # convert to tensors\n        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n        if self.isTrain:\n            output[\"target\"] = torch.tensor(output[\"target\"], dtype=torch.long)\n\n        return output\n\ncollate_fn = Collate(CONFIG[\"tokenizer\"], isTrain=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:02:25.746274Z","iopub.execute_input":"2022-08-02T06:02:25.747154Z","iopub.status.idle":"2022-08-02T06:02:25.759714Z","shell.execute_reply.started":"2022-08-02T06:02:25.747116Z","shell.execute_reply":"2022-08-02T06:02:25.758963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = FeedBackDataset(df, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                         num_workers=2, shuffle=False,collate_fn=collate_fn, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:02:25.761321Z","iopub.execute_input":"2022-08-02T06:02:25.762161Z","iopub.status.idle":"2022-08-02T06:02:25.813721Z","shell.execute_reply.started":"2022-08-02T06:02:25.762126Z","shell.execute_reply":"2022-08-02T06:02:25.812671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\n\nclass MeanMaxPooling(nn.Module):\n    def __init__(self):\n        super(MeanMaxPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        mean_pooling_embeddings = torch.mean(last_hidden_state, 1)\n        _, max_pooling_embeddings = torch.max(last_hidden_state, 1)\n        mean_max_embeddings = torch.cat((mean_pooling_embeddings, max_pooling_embeddings), 1)\n        return mean_max_embeddings\n\n\nclass LSTMPooling(nn.Module):\n    def __init__(self, num_layers, hidden_size, hiddendim_lstm):\n        super(LSTMPooling, self).__init__()\n        self.num_hidden_layers = num_layers\n        self.hidden_size = hidden_size\n        self.hiddendim_lstm = hiddendim_lstm\n        self.lstm = nn.LSTM(self.hidden_size, self.hiddendim_lstm, batch_first=True)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, all_hidden_states):\n        ## forward\n        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n                                     for layer_i in range(1, self.num_hidden_layers + 1)], dim=-1)\n        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n        out, _ = self.lstm(hidden_states, None)\n        out = self.dropout(out[:, -1, :])\n        return out\n\n\nclass WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights=None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n            torch.tensor([1] * (num_hidden_layers + 1 - layer_start), dtype=torch.float)\n        )\n\n    def forward(self, all_hidden_states):\n        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n        return weighted_average","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:02:25.817013Z","iopub.execute_input":"2022-08-02T06:02:25.817286Z","iopub.status.idle":"2022-08-02T06:02:25.834445Z","shell.execute_reply.started":"2022-08-02T06:02:25.817261Z","shell.execute_reply":"2022-08-02T06:02:25.833603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedBackModel(nn.Module):\n    def __init__(self, model_name):\n        super(FeedBackModel, self).__init__()\n        self.cfg = CONFIG\n        self.config = AutoConfig.from_pretrained(model_name,output_hidden_states=True)\n        self.model = AutoModel.from_pretrained(model_name,config=self.config)\n        # gradient checkpointing  梯度检查点\n        if CONFIG['gradient_checkpoint']:\n            self.model.gradient_checkpointing_enable()\n            print(f\"Gradient Checkpointing: {self.model.is_gradient_checkpointing}\")\n        self.bilstm = nn.LSTM(self.config.hidden_size, (self.config.hidden_size) // 2, num_layers=2,\n                              dropout=self.config.hidden_dropout_prob, batch_first=True,\n                              bidirectional=True)\n\n        self.dropout = nn.Dropout(0.2)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n        #-----------------\n        #self.pooler=WeightedLayerPooling(self.config.num_hidden_layers)\n        #self.pooler=MeanPooling()\n\n        self.output = nn.Sequential(\n            nn.Linear(self.config.hidden_size*4, CONFIG['num_classes'])\n            # nn.Linear(256, self.cfg.target_size)\n        )\n\n    def loss(self, outputs, targets):\n        loss_fct = nn.CrossEntropyLoss()\n        loss = loss_fct(outputs, targets)\n        return loss\n\n    def monitor_metrics(self, outputs, targets):\n        device = targets.get_device()\n        # print(outputs)\n        # print(targets)\n        mll = log_loss(\n            targets.cpu().detach().numpy(),\n            softmax(outputs.cpu().detach().numpy()),\n            labels=[0, 1, 2],\n        )\n        return mll\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    # def forward(self, ids, mask):\n    #     out = self.model(input_ids=ids, attention_mask=mask,\n    #                      output_hidden_states=False)\n    #     out = self.pooler(out.last_hidden_state, mask)\n    #     out = self.drop(out)\n    #     outputs = self.fc(out)\n    #     return outputs\n    def forward(self, ids, mask, token_type_ids=None, targets=None):\n        if token_type_ids:\n            transformer_out = self.model(ids, mask, token_type_ids)\n        else:\n            transformer_out = self.model(ids, mask)\n\n        # LSTM/GRU header\n        # all_hidden_states = torch.stack(transformer_out[1])\n        # sequence_output = self.pooler(all_hidden_states)3\n        all_hidden_states = torch.stack(transformer_out[1])\n\n        concatenate_pooling = torch.cat(\n            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]), -1\n        )\n        sequence_output = concatenate_pooling[:, 0]\n        # simple CLS\n        #transformer_out = self.pooler(transformer_out.last_hidden_state,mask)\n        #sequence_output = transformer_out\n        #sequence_output = transformer_out[0][:, 0, :]\n        #all_hidden_states = torch.stack(transformer_out[1])\n        #sequence_output=self.pooler(all_hidden_states)\n\n        # Main task\n        logits1 = self.output(self.dropout1(sequence_output))\n        logits2 = self.output(self.dropout2(sequence_output))\n        logits3 = self.output(self.dropout3(sequence_output))\n        logits4 = self.output(self.dropout4(sequence_output))\n        logits5 = self.output(self.dropout5(sequence_output))\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n\n        if targets is not None:\n            metric = self.monitor_metrics(logits, targets)\n            return logits\n\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:02:25.836073Z","iopub.execute_input":"2022-08-02T06:02:25.837055Z","iopub.status.idle":"2022-08-02T06:02:25.855665Z","shell.execute_reply.started":"2022-08-02T06:02:25.836969Z","shell.execute_reply":"2022-08-02T06:02:25.854851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_fn(model, dataloader, device):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    PREDS = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        ids = data['input_ids'].to(device, dtype = torch.long)\n        mask = data['attention_mask'].to(device, dtype = torch.long)\n        \n        outputs = model(ids, mask)\n        outputs = F.softmax(outputs, dim=1)\n        PREDS.append(outputs.cpu().detach().numpy()) \n    \n    PREDS = np.concatenate(PREDS)\n    gc.collect()\n    \n    return PREDS","metadata":{"papermill":{"duration":23.005949,"end_time":"2022-06-29T02:56:06.518411","exception":false,"start_time":"2022-06-29T02:55:43.512462","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:02:25.857033Z","iopub.execute_input":"2022-08-02T06:02:25.857468Z","iopub.status.idle":"2022-08-02T06:02:25.869596Z","shell.execute_reply.started":"2022-08-02T06:02:25.857432Z","shell.execute_reply":"2022-08-02T06:02:25.868832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference(model_paths, dataloader, device):\n    final_preds = []\n    for i, path in enumerate(model_paths):\n        model = FeedBackModel(CONFIG['model_name'])\n        model.to(CONFIG['device'])\n        model.load_state_dict(torch.load(path))\n        \n        print(f\"Getting predictions for model {i+1}\")\n        preds = valid_fn(model, dataloader, device)\n        final_preds.append(preds)\n        del model, preds\n        torch.cuda.empty_cache()    \n        gc.collect()\n    \n#     final_preds = np.array(final_preds)\n#     final_preds = np.mean(final_preds, axis=0)\n\n\n    return final_preds","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:02:25.870757Z","iopub.execute_input":"2022-08-02T06:02:25.871189Z","iopub.status.idle":"2022-08-02T06:02:25.881190Z","shell.execute_reply.started":"2022-08-02T06:02:25.871155Z","shell.execute_reply":"2022-08-02T06:02:25.880353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_predicts = inference(MODEL_PATHS, test_loader, CONFIG['device'])","metadata":{"papermill":{"duration":0.042065,"end_time":"2022-06-29T02:56:06.57446","exception":false,"start_time":"2022-06-29T02:56:06.532395","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:02:25.882597Z","iopub.execute_input":"2022-08-02T06:02:25.882974Z","iopub.status.idle":"2022-08-02T06:04:10.159713Z","shell.execute_reply.started":"2022-08-02T06:02:25.882940Z","shell.execute_reply":"2022-08-02T06:04:10.158816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_predicts","metadata":{"papermill":{"duration":0.019897,"end_time":"2022-06-29T02:56:06.606774","exception":false,"start_time":"2022-06-29T02:56:06.586877","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:04:10.276149Z","iopub.execute_input":"2022-08-02T06:04:10.276709Z","iopub.status.idle":"2022-08-02T06:04:10.286554Z","shell.execute_reply.started":"2022-08-02T06:04:10.276657Z","shell.execute_reply":"2022-08-02T06:04:10.285628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":3.524253,"end_time":"2022-06-29T02:56:10.143211","exception":false,"start_time":"2022-06-29T02:56:06.618958","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_ineffective = []\nbase_effective = []\nbase_adequate = []\n\nfor x in base_predicts:\n    base_ineffective.append(x[:, 2])\n    base_adequate.append(x[:, 0])\n    base_effective.append(x[:, 1])","metadata":{"papermill":{"duration":0.022375,"end_time":"2022-06-29T02:56:10.178553","exception":false,"start_time":"2022-06-29T02:56:10.156178","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:04:10.288124Z","iopub.execute_input":"2022-08-02T06:04:10.289164Z","iopub.status.idle":"2022-08-02T06:04:10.295450Z","shell.execute_reply.started":"2022-08-02T06:04:10.289117Z","shell.execute_reply":"2022-08-02T06:04:10.294544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_ineffective = pd.DataFrame(base_ineffective).T\n\nshow_gradient(\n    base_ineffective,\n    N_ROW)","metadata":{"papermill":{"duration":0.07251,"end_time":"2022-06-29T02:56:10.263745","exception":false,"start_time":"2022-06-29T02:56:10.191235","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:04:10.296811Z","iopub.execute_input":"2022-08-02T06:04:10.297442Z","iopub.status.idle":"2022-08-02T06:04:10.329615Z","shell.execute_reply.started":"2022-08-02T06:04:10.297402Z","shell.execute_reply":"2022-08-02T06:04:10.328796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_adequate = pd.DataFrame(base_adequate).T\n\nshow_gradient(\n    base_adequate,\n    N_ROW)","metadata":{"papermill":{"duration":0.095428,"end_time":"2022-06-29T02:56:10.3919","exception":false,"start_time":"2022-06-29T02:56:10.296472","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:04:10.330913Z","iopub.execute_input":"2022-08-02T06:04:10.331251Z","iopub.status.idle":"2022-08-02T06:04:10.361485Z","shell.execute_reply.started":"2022-08-02T06:04:10.331217Z","shell.execute_reply":"2022-08-02T06:04:10.360730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_effective = pd.DataFrame(base_effective).T\n\nshow_gradient(\n    base_effective,\n    N_ROW)","metadata":{"papermill":{"duration":0.081635,"end_time":"2022-06-29T02:56:10.511529","exception":false,"start_time":"2022-06-29T02:56:10.429894","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:04:10.364221Z","iopub.execute_input":"2022-08-02T06:04:10.364527Z","iopub.status.idle":"2022-08-02T06:04:10.390927Z","shell.execute_reply.started":"2022-08-02T06:04:10.364502Z","shell.execute_reply":"2022-08-02T06:04:10.390242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 roberta\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.discourse = df['discourse'].values\n        self.essay = df['essay'].values\n        \n    def __len__(self):\n        return len(self.discourse)\n    \n    def __getitem__(self, item):\n        discourse = self.discourse[item]\n        essay = self.essay[item]\n        \n        inputs = prepare_input(self.cfg, discourse, essay)\n        \n        return inputs\n        \nclass FeedBackModel(nn.Module):\n    def __init__(self, model_path):\n        super(FeedBackModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_path)\n        self.linear = nn.Linear(768, 3)\n\n    def forward(self, inputs):\n        last_hidden_states = self.model(**inputs)[0][:, 0, :]\n        outputs = self.linear(last_hidden_states)\n        \n        return outputs\nmodel_list = pickle.load(\n    open(\"../input/feedback-roberta-ep1/roberta_modellist_ep2.pkl\", \"rb\")\n)\n\nclass CFG:\n    path = \"../input/roberta-base/\"\n    n_fold = 5\n    batch = 16\n    max_len = 512\n    num_workers = 2\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path)\ndf = test_origin.copy()\n\ntxt_sep = \" \"\ndf['discourse'] = df['discourse_type'].str.lower().str.strip() + txt_sep \\\n                + df['discourse_text'].str.lower().str.strip()\n\ndf['essay'] = df['essay_id'].transform(fetch_essay, txt_dir='test').str.lower().str.strip()\ndf.head()\n\n\ntest_dataset = TestDataset(CFG, df)\ntest_loader = DataLoader(test_dataset, batch_size=CFG.batch,\n                         shuffle=False, num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\nroberta_predicts = []\nfor i in range(CFG.n_fold):\n    model = model_list[i]\n    \n    prediction = inference_fn(test_loader, model, DEVICE)\n    roberta_predicts.append(prediction)\n    \n    del model, prediction\n    torch.cuda.empty_cache()    \n    gc.collect()\n    \ndel model_list\ngc.collect()\n\nrob_ineffective = []\nrob_effective = []\nrob_adequate = []\n\nfor x in roberta_predicts:\n    rob_ineffective.append(x[:, 0])\n    rob_adequate.append(x[:, 1])\n    rob_effective.append(x[:, 2])\n\n# list -> dataframe\nrob_ineffective = pd.DataFrame(rob_ineffective).T\nrob_adequate = pd.DataFrame(rob_adequate).T\nrob_effective = pd.DataFrame(rob_effective).T","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:04:10.392368Z","iopub.execute_input":"2022-08-02T06:04:10.392845Z","iopub.status.idle":"2022-08-02T06:04:37.978132Z","shell.execute_reply.started":"2022-08-02T06:04:10.392772Z","shell.execute_reply":"2022-08-02T06:04:37.977054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roberta_predicts","metadata":{"execution":{"iopub.status.busy":"2022-08-02T06:04:37.979861Z","iopub.execute_input":"2022-08-02T06:04:37.980245Z","iopub.status.idle":"2022-08-02T06:04:37.989241Z","shell.execute_reply.started":"2022-08-02T06:04:37.980204Z","shell.execute_reply":"2022-08-02T06:04:37.988462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Create submission","metadata":{"papermill":{"duration":0.024077,"end_time":"2022-06-29T02:56:10.562061","exception":false,"start_time":"2022-06-29T02:56:10.537984","status":"completed"},"tags":[]}},{"cell_type":"code","source":"level_names = ['deberta', 'base','rob']\n\nineffective_ = pd.concat(\n    [deb_ineffective, base_ineffective,rob_ineffective],\n    keys=level_names, axis=1\n)\n\nadequate_ = pd.concat(\n    [deb_adequate, base_adequate,rob_adequate],\n    keys=level_names, axis=1\n)\n\neffective_ = pd.concat(\n    [deb_effective, base_effective,rob_effective],\n    keys=level_names, axis=1\n)","metadata":{"papermill":{"duration":0.046338,"end_time":"2022-06-29T02:56:10.633434","exception":false,"start_time":"2022-06-29T02:56:10.587096","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:04:37.990481Z","iopub.execute_input":"2022-08-02T06:04:37.991403Z","iopub.status.idle":"2022-08-02T06:04:38.007147Z","shell.execute_reply.started":"2022-08-02T06:04:37.991367Z","shell.execute_reply":"2022-08-02T06:04:38.006272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_gradient(\n    ineffective_,\n    N_ROW\n)","metadata":{"papermill":{"duration":0.137257,"end_time":"2022-06-29T02:56:10.79562","exception":false,"start_time":"2022-06-29T02:56:10.658363","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:04:38.008322Z","iopub.execute_input":"2022-08-02T06:04:38.008723Z","iopub.status.idle":"2022-08-02T06:04:38.090483Z","shell.execute_reply.started":"2022-08-02T06:04:38.008677Z","shell.execute_reply":"2022-08-02T06:04:38.089605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_gradient(\n    adequate_,\n    N_ROW\n)","metadata":{"papermill":{"duration":0.102402,"end_time":"2022-06-29T02:56:10.923467","exception":false,"start_time":"2022-06-29T02:56:10.821065","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:04:38.092231Z","iopub.execute_input":"2022-08-02T06:04:38.092975Z","iopub.status.idle":"2022-08-02T06:04:38.187062Z","shell.execute_reply.started":"2022-08-02T06:04:38.092924Z","shell.execute_reply":"2022-08-02T06:04:38.186323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_gradient(\n    effective_,\n    N_ROW\n)","metadata":{"papermill":{"duration":0.079449,"end_time":"2022-06-29T02:56:11.019182","exception":false,"start_time":"2022-06-29T02:56:10.939733","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:04:38.188262Z","iopub.execute_input":"2022-08-02T06:04:38.188604Z","iopub.status.idle":"2022-08-02T06:04:38.260824Z","shell.execute_reply.started":"2022-08-02T06:04:38.188571Z","shell.execute_reply":"2022-08-02T06:04:38.259943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = submission_origin.copy()\n\nw_ = [.30,.60,.10]  \nd_ = [('Ineffective', ineffective_),\n      ('Adequate', adequate_),\n      ('Effective', effective_)]\n\nfor x in d_:\n    col_name, df = x\n    submission[col_name] = pd.DataFrame(\n        {col: df[col].mean(axis=1) for col in level_names}\n    ).mul(w_).sum(axis=1)    \n\nsubmission.head(N_ROW)","metadata":{"papermill":{"duration":0.042962,"end_time":"2022-06-29T02:56:11.078725","exception":false,"start_time":"2022-06-29T02:56:11.035763","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:04:38.262367Z","iopub.execute_input":"2022-08-02T06:04:38.262763Z","iopub.status.idle":"2022-08-02T06:04:38.288712Z","shell.execute_reply.started":"2022-08-02T06:04:38.262732Z","shell.execute_reply":"2022-08-02T06:04:38.287862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 0\ta261b6e14276\t0.0102\t0.3887\t0.5911\n# 1\t5a88900e7dc1\t0.0309\t0.8405\t0.1185\n# 2\t9790d835736b\t0.0217\t0.6997\t0.2686\n# 3\t75ce6d68b67b\t0.0512\t0.6365\t0.3023\n# 4\t93578d946723\t0.0399\t0.6053\t0.3448\n# 5\t2e214524dbe3\t0.0099\t0.3721\t0.6080\n# 6\t84812fc2ab9f\t0.0084\t0.2796\t0.7020\n# 7\tc668ff840720\t0.0171\t0.5888\t0.3841\n# 8\t739a6d00f44a\t0.0178\t0.4030\t0.5692\n# 9\tbcfae2c9a244\t0.0132\t0.6373\t0.3395","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.021768,"end_time":"2022-06-29T02:56:11.116966","exception":false,"start_time":"2022-06-29T02:56:11.095198","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:04:38.290182Z","iopub.execute_input":"2022-08-02T06:04:38.290538Z","iopub.status.idle":"2022-08-02T06:04:38.294714Z","shell.execute_reply.started":"2022-08-02T06:04:38.290505Z","shell.execute_reply":"2022-08-02T06:04:38.293809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv',index=False)","metadata":{"papermill":{"duration":0.027941,"end_time":"2022-06-29T02:56:11.160924","exception":false,"start_time":"2022-06-29T02:56:11.132983","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-02T06:04:38.295835Z","iopub.execute_input":"2022-08-02T06:04:38.296968Z","iopub.status.idle":"2022-08-02T06:04:38.310340Z","shell.execute_reply.started":"2022-08-02T06:04:38.296932Z","shell.execute_reply":"2022-08-02T06:04:38.309548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.01552,"end_time":"2022-06-29T02:56:11.19256","exception":false,"start_time":"2022-06-29T02:56:11.17704","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}